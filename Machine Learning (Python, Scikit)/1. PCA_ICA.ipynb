{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshy105/Uni_luebeck/blob/main/ML_exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGWxm40p8MkH"
      },
      "source": [
        "# PCA, ICA and Sparse Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSnp921f8MkK"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhtoH7eW8MkL"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs5450.git'\n",
        "    os.chdir('cs5450')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRuRoiKD8MkR"
      },
      "source": [
        "## Applying unsupervised methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFAXbfVj8MkS"
      },
      "source": [
        "Import and helper functions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6JhtU_98MkT",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from sklearn.decomposition import PCA, FastICA, DictionaryLearning\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def random_patches(data, img_height, img_width, n_patches, patch_size):\n",
        "    # RANDOMPATCHES draw random square-shaped patches from the data.\n",
        "    #\n",
        "    # INPUT:\n",
        "    #   data : ndarray containing the data (i.e. mnist or olivetti)\n",
        "    #   img_height : height of the images\n",
        "    #   img_width : width of the images\n",
        "    #   n_patches : number of patches to be drawn\n",
        "    #   patch_size : size (both width and height) of the square-shaped patches\n",
        "    #\n",
        "    # OUTPUT:\n",
        "    #   patches : random patches\n",
        "\n",
        "    # reshape data 2d -> 3d\n",
        "    n_images = data.shape[0]\n",
        "    data = data.reshape(n_images, img_height, img_width)\n",
        "\n",
        "    # extract random patches\n",
        "    patches = []#np.zeros((n_patches, patch_size*patch_size))\n",
        "    for i in range(n_patches):\n",
        "        patch_top = random.randint(0, img_height - patch_size)\n",
        "        patch_left = random.randint(0, img_width - patch_size)\n",
        "        p = data[random.randint(0, n_images-1), patch_top:patch_top+patch_size, patch_left:patch_left+patch_size]\n",
        "        patches.append(p)\n",
        "        #patches[i,:] = p[:]\n",
        "        \n",
        "    patches = np.stack(patches)\n",
        "    patches = patches.reshape(patches.shape[0], -1) #flatten each patch\n",
        "        \n",
        "    return patches\n",
        "\n",
        "def show_in_grid(images, height, width):\n",
        "    # flatten images if necessary\n",
        "    images = images.reshape(images.shape[0], -1)\n",
        "    \n",
        "    # normalize patches\n",
        "    images = images / (np.abs(images).max(axis=1, keepdims=True)+1e-6)\n",
        "    \n",
        "    # reshape into images\n",
        "    images = images.reshape(-1, height, width)\n",
        "    \n",
        "    # make images fit into a rectangular area\n",
        "    grid_width = math.ceil(math.sqrt(images.shape[0]))\n",
        "    grid_height = math.ceil(images.shape[0] / grid_width)\n",
        "    \n",
        "    empty_cells = grid_width * grid_height - images.shape[0]\n",
        "    \n",
        "    # fill empty cells\n",
        "    if empty_cells > 0:\n",
        "        padding = np.zeros((empty_cells, height, width))\n",
        "        images = np.concatenate((images, padding))\n",
        "        \n",
        "    # rearrange basis into grid and also switch width and height so x and y axis are not switched\n",
        "    images = images.reshape(grid_height, grid_width, height, width)\n",
        "    images = images.transpose(0, 3, 1, 2)\n",
        "    \n",
        "    plt.figure(figsize = (10,10))\n",
        "    plt.imshow(images.reshape(grid_height*height, grid_width*width), cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49vhi-h18Mkb"
      },
      "outputs": [],
      "source": [
        "## Define variables\n",
        "\n",
        "#  dataset : relative path to the .npz file containing the data\n",
        "dataset = 'data/mnist.npz' #'data/olivetti.npz'\n",
        "\n",
        "#  n_patches : number of random patches to extract from the images\n",
        "n_patches = 10000\n",
        "#  patch_size : size (both width and height) of the square-shaped patches\n",
        "patch_size = 25\n",
        "\n",
        "#  n_basis : number of new basis vectors\n",
        "n_basis = 100\n",
        "\n",
        "#  method : string describing whether to perform pca, ica, or sc\n",
        "method = 'sc'\n",
        "\n",
        "#  n_iterations : number of iterations (only for sparse coding)\n",
        "n_iterations = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU1kPQI68Mkf"
      },
      "outputs": [],
      "source": [
        "## Compute and show the principal components\n",
        "\n",
        "#  load dataset\n",
        "given_dataset = np.load(dataset)\n",
        "data = given_dataset['data']\n",
        "\n",
        "#  convert data to floating point (double precision)\n",
        "data = data.astype('double')\n",
        "# print(data)\n",
        "\n",
        "#  get number of images (observations) and number of pixels (features)\n",
        "n_images, n_pixels = data.shape\n",
        "# print(data.shape)\n",
        "\n",
        "#  get height and width of the images\n",
        "img_height = int(np.sqrt(n_pixels))\n",
        "img_width = img_height\n",
        "assert isinstance(img_height, int) and isinstance(img_width, int), \"Image dimensions need to be integers.\"\n",
        "\n",
        "#  enforce mean-free data vectors\n",
        "mean_free_data = data - np.mean(data, axis=0)\n",
        "# print(mean_free_data)\n",
        "\n",
        "#  draw random patches from data.\n",
        "patches = random_patches(mean_free_data, img_height, img_width, n_patches, patch_size)\n",
        "\n",
        "#  perform the chosen unsupervised method on the drawn patches\n",
        "if method == 'pca':\n",
        "    pca = PCA(n_basis)\n",
        "    basis = pca.fit(patches).components_\n",
        "elif method == 'ica':\n",
        "    ica = FastICA(n_basis)\n",
        "    basis = ica.fit(patches).components_\n",
        "elif method == 'sc':\n",
        "    sc = DictionaryLearning(n_basis, n_jobs=-1, verbose=True)\n",
        "    basis = sc.fit_transform(patches)\n",
        "\n",
        "#  show the new set of basis vectors (only the first n_basis)\n",
        "show_in_grid(basis[:n_basis], patch_size, patch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pE1226K8Mkj"
      },
      "outputs": [],
      "source": [
        "## Use different subsets of the new basis vectors to reconstruct a random patch\n",
        "from sklearn.decomposition import sparse_encode\n",
        "\n",
        "#  draw a random patch\n",
        "random_patch = random_patches(mean_free_data, img_height, img_width, 1, patch_size)\n",
        "# print(random_patch)\n",
        "\n",
        "#  reconstruct the random patch\n",
        "reconstructions = []\n",
        "for i in range(n_basis):\n",
        "    sub_basis = basis[0:i+1]\n",
        "    \n",
        "    if method == 'sc':\n",
        "      sparse_signal = sparse_encode(random_patch, sub_basis)\n",
        "      part = np.matmul(sparse_signal, sub_basis)\n",
        "      reconstructions.append(part)\n",
        "    else:\n",
        "        proj = np.matmul(np.linalg.pinv(sub_basis), np.matmul(sub_basis, np.transpose(random_patch)))\n",
        "        reconstructions.append(proj)\n",
        "    \n",
        "reconstructions = np.stack(reconstructions)\n",
        "\n",
        "#  show the reconstructed patch\n",
        "show_in_grid(reconstructions, patch_size, patch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82AN1L7Q8Mko"
      },
      "source": [
        "whitened data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NazVD1vN8Mkq"
      },
      "outputs": [],
      "source": [
        "#%  epsilon : avoid negative numbers while computing stdDeviations\n",
        "epsilon = 0.1\n",
        "\n",
        "#  perform principal component analysis (help pca)\n",
        "basis = -1 #get basis vector components\n",
        "\n",
        "#  compute the covariance matrix of the patches\n",
        "covariance = -1\n",
        "\n",
        "#  compute diagonal matrix S containing the standard deviations (c.f. page 6)\n",
        "std_deviations = -1\n",
        "\n",
        "#  pca whitening (c.f. equation 1.3)\n",
        "xPCAwhite = -1\n",
        "\n",
        "#  zca whitening\n",
        "xZCAwhite = -1\n",
        "\n",
        "#  visualize the whitened data\n",
        "# show_in_grid(xZCAwhite[:n_basis], patch_size, patch_size)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "ML exercise_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PT3",
      "language": "python",
      "name": "pt3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
