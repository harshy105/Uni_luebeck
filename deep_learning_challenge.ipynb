{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": " deep_learning_challenge.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshy105/Uni_luebeck/blob/main/deep_learning_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ipLoPd_XlOk"
      },
      "source": [
        "# Deep Learning Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1NvDcWwXlOm"
      },
      "source": [
        "## Preamble\n",
        "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W99_1m8lXlOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df128c4b-1b0d-49da-d7e2-e3505500d65b"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "  if os.getcwd() == '/content':\n",
        "    !git clone 'https://github.com/inb-uni-luebeck/cs5450.git'\n",
        "    os.chdir('cs5450')\n",
        "\n",
        "#making sure livelossplot is installed\n",
        "try:\n",
        "    import livelossplot\n",
        "except ModuleNotFoundError:\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install livelossplot==0.4.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'cs5450'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 32 (delta 9), reused 29 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n",
            "Collecting livelossplot==0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/f6/0618c30078f9c1e4b2cd84f1ea6bb70c6615070468b75b0d934326107bcd/livelossplot-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.1) (3.2.2)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.4.1) (5.3.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.1) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.1) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.1) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->livelossplot==0.4.1) (1.19.4)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (5.3.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (4.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (2.11.2)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (0.9.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (1.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (5.0.8)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (4.7.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook->livelossplot==0.4.1) (5.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->livelossplot==0.4.1) (1.15.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client>=5.2.0->notebook->livelossplot==0.4.1) (20.0.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->notebook->livelossplot==0.4.1) (5.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->livelossplot==0.4.1) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->livelossplot==0.4.1) (0.6.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->livelossplot==0.4.1) (2.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->livelossplot==0.4.1) (4.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (0.6.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (1.4.3)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (2.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook->livelossplot==0.4.1) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (51.0.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (4.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.4.1) (20.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook->livelossplot==0.4.1) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->notebook->livelossplot==0.4.1) (0.2.5)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJMRgV5gXlOo"
      },
      "source": [
        "## Setup\n",
        "This exercise can utilize GPU acceleration. If you are using Google Colab you can enable access to a cloud GPU by selecting from the menu above: \n",
        "\n",
        "**Runtime > Change runtime type > Hardware accelerator > GPU**\n",
        "\n",
        "If you are running this notebook on your own machine, GPU acceleration is available if you have an Nvidia GPU and a CUDA-enabled driver installed. Otherwise calculations will run on the CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvB-hyxeXlOo"
      },
      "source": [
        "## Iceberg vs. Ship\n",
        "In this exercise, we will use deep learning to solve a real-world problem: classifying whether a given grayscale representation of a radar image is a ship or an iceberg. To make things more interesting, this exercise sheet will be carried out as a competition.\n",
        "\n",
        "Besides the labeled training set of $1000$ images, an unlabeled test set of $604$ images is provided. The dimension of each grayscale image is $30 \\times 30$ pixels. An example image of both a ship and an iceberg is given in the figure below.\n",
        "  \n",
        "The next cells provide code to get you started. It contains a fully functional implementation of a simple convolutional neural network (CNN). It has a baseline accuracy of $79.50\\%$ on the fixed validation set. The architecture of the network is given in table below. Your task is to extend the code, thus optimizing the classification capabilities of the network.\n",
        "\n",
        "To evaluate the performance of your network, the average **log loss** is used. For a single sample $\\vec{x}$ with true binary label $y \\in \\left\\{ 0,1 \\right\\}$ and an estimated posterior probability $p \\in \\left[ 0,1 \\right]$, the log loss is defined as:\n",
        "  \n",
        "  $ E \\left( \\vec{x} \\right) = - \\left[ y \\log p + \\left( 1 - y \\right) \\log \\left( 1 - p \\right) \\right].$\n",
        "\n",
        "Your final submission should be a $604$-dimensional vector: the probability that the given sample is an iceberg (label $1$) for each sample of the test set. \n",
        "\n",
        "Iceberg | Ship    \n",
        "- | - \n",
        "<img src=\"https://github.com/inb-uni-luebeck/cs5450/blob/master/data/iceberg.jpg?raw=1\" width=\"300\"/> | <img src=\"https://github.com/inb-uni-luebeck/cs5450/blob/master/data/ship.jpg?raw=1\" width=\"300\"/>\n",
        "\n",
        "| Layer       | Kernel | Stride |    Shape   | #Params |\n",
        "|-------------|:------:|:------:|:----------:|:-------:|\n",
        "| Input       |    -   |    -   |      -     |    0    |\n",
        "| Convolution |  (3,3) |  (1,1) |  (30,30,1) |   160   |\n",
        "| Pooling     |  (2,2) |  (2,2) | (28,28,16) |    0    |\n",
        "| Linear      |    -   |    -   |     (2)    |   6274  |\n",
        "\n",
        "\n",
        "## Hints\n",
        "In case you are struggeling with the task, here are some helpful tips and hints:\n",
        "1. Useful pytorch [overview](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
        "2. [Augment](https://pytorch.org/docs/stable/torchvision/transforms.html) the training data (e.g. mirroring).\n",
        "3. Make the network deeper (e.g. more convolution and pooling layers)\n",
        "4. Regularize your network (e.g. with dropout layers).\n",
        "5. Use cross-validation instead of a fixed train-validation split.\n",
        "6. Train multiple networks and combine them in an ensemble.\n",
        "7. Post-process your posterior probabilities (e.g. is log 0 a good idea?)\n",
        "\n",
        "## Further notes and remarks:\n",
        "1. Each team has to choose a team name. This will be the anonymous identifier when\n",
        "the competition results are published.\n",
        "2. To reduce computation time, the networks are trained on the graphics processing\n",
        "unit (GPU) of the pool computers. In case you want to train them on your personal computer without a GPU, simply change the line ’ExecutionEnvironment’,\n",
        "’gpu’ to ’cpu’ inside the trainingOptions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7WqfrecXlOq"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from livelossplot import PlotLosses\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_LmDvphXlOq"
      },
      "source": [
        "def load_data(path):\n",
        "    X = np.load(path)[...,0,:].astype('float32')\n",
        "    X -= X.min()\n",
        "    X = (255.*X/X.max()).astype('uint8')\n",
        "    X = np.swapaxes(X,0,2)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XekHr1BSXlOq"
      },
      "source": [
        "default_aug = test_augmentation = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "\n",
        "class IcebergDataset(Dataset):\n",
        "    def __init__(self, X, Y=None, augmentation=default_aug, is_test=False):\n",
        "        if Y is None:\n",
        "            assert is_test\n",
        "            \n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        \n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.augmentation = augmentation\n",
        "        \n",
        "        self.is_test = is_test\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.X[index, ...]\n",
        "        \n",
        "        x = self.augmentation(x)\n",
        "        \n",
        "        if self.is_test:\n",
        "            return x\n",
        "        y = self.Y[index]\n",
        "        y = torch.tensor(y).float()\n",
        "        \n",
        "        return x, y\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anedMIrhXlOr"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# ------------Architecture------------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "def make_conv_layer(in_dim, out_dim, kernel_size=3, **kwargs):\n",
        "    # this \"conv_layer\" is actually a composition of convolution-layer, relu and batch-normalization\n",
        "    conv_layer = nn.Sequential()\n",
        "    \n",
        "    # NOTE: kwargs could be a dictionary like {'padding':1}\n",
        "    # you can call the function like e.g.:\n",
        "    # make_conv_layer(in_dim, out_dim, kernel_size=3, **{'padding':1}) \n",
        "    conv_layer.add_module('conv', nn.Conv2d(in_dim, out_dim, kernel_size, **kwargs))\n",
        "    conv_layer.add_module('relu', nn.ReLU())\n",
        "    conv_layer.add_module('batch_norm', nn.BatchNorm2d(out_dim))\n",
        "    \n",
        "    return conv_layer\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # in: (b, 1, 30, 30) b= batch_size\n",
        "        self.conv_0 = make_conv_layer(1, 16, 3)\n",
        "        # in: (b, 16, 28, 28)\n",
        "        self.pool_0 = nn.MaxPool2d(2, 2) # kernel -> (2, 2), stride -> (2, 2)\n",
        "        \n",
        "        # in: (b, 16, 14, 14) -> will be flattened to (b, 16*14*14) = (b, 3136)\n",
        "        self.linear = nn.Linear(16*14*14, 1)\n",
        "        # ouput: (b, 1) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # forward is the function that is called when using prediction = model(x)\n",
        "        \n",
        "        x = self.conv_0(x)\n",
        "        x = self.pool_0(x)\n",
        "\n",
        "        # flatten before fully connected layer \n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x[:,0]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4ZtB4xcXlOs"
      },
      "source": [
        "def get_optimizer(opt_id, parameters, learning_rate, **kwargs):\n",
        "    if opt_id == 'SGD':\n",
        "        optimizer = optim.SGD(parameters,\n",
        "                              lr=learning_rate,\n",
        "                              momentum=kwargs.get('momentum', .9),\n",
        "                              weight_decay=kwargs.get('weight_decay', 0.)\n",
        "                              )\n",
        "    elif opt_id == 'Adam':\n",
        "        optimizer = optim.Adam(\n",
        "            parameters,\n",
        "            lr=learning_rate,\n",
        "            weight_decay=kwargs.get('weight_decay', 0.)\n",
        "        )\n",
        "    \n",
        "    return optimizer\n",
        "        \n",
        "def get_scheduler(lr_steps, epochs, optimizer, gamma=.1):\n",
        "    assert lr_steps < epochs, 'Epochs must be greater than lr_steps'\n",
        "    step_size = epochs // lr_steps\n",
        "\n",
        "    scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size,\n",
        "        gamma=gamma, last_epoch=-1\n",
        "    )\n",
        "\n",
        "    return scheduler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWUPxE3UXlOt"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# ------------Hyperparameters---------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "# Data set params: run the Train & Test Data cell for changes here to take place!\n",
        "batch_size = 32\n",
        "perc_test_set = .2\n",
        "\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 0.\n",
        "num_sched_steps = 2\n",
        "epochs = 120\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFMRSlnXXlOt"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# ------------Train & Test Data-------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "# images need to be real uint8 images in [0, 255]\n",
        "\n",
        "X = load_data('data/train_data.npy')\n",
        "Y = np.load('data/train_labels.npy')[:,0]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=perc_test_set, random_state=42)\n",
        "\n",
        "mu = (X_train/255).mean()\n",
        "sigma = (X_train/255).std()\n",
        "normalize = transforms.Normalize(mean=[mu,], std=[sigma,])\n",
        "\n",
        "train_augmentation = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "            \n",
        "        ])\n",
        "\n",
        "\n",
        "test_augmentation = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "\n",
        "data_loader_train = DataLoader(\n",
        "    IcebergDataset(X_train,Y_train, train_augmentation),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "data_loader_val = DataLoader(\n",
        "    IcebergDataset(X_test,Y_test, test_augmentation),\n",
        "    batch_size=batch_size,\n",
        "    num_workers=0,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": data_loader_train,\n",
        "    \"validation\": data_loader_val\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BClHn1iEXlOt"
      },
      "source": [
        "# TODO: \n",
        "# early stopping\n",
        "# submission code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRH-fNCEXlOu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "fd134b03-5a5b-4645-8bef-8aa18522a5c1"
      },
      "source": [
        "# train on cuda if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "liveloss = PlotLosses(plot_extrema=False)\n",
        " \n",
        "#-------------------------------------------------\n",
        "# ------------Setup-----------------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "model = ConvNet().cuda#to(device)\n",
        "\n",
        "#optimizer = get_optimizer(\n",
        "#    'SGD', model.parameters(), learning_rate,\n",
        "#    **{'weight_decay':weight_decay, 'momentum':0.9}\n",
        "#)\n",
        "\n",
        "#optimizer = optim.Adam(list(model.parameters()),learning_rate)\n",
        "\n",
        "#scheduler = get_scheduler(num_sched_steps, epochs, optimizer)\n",
        "scheduler = None\n",
        "\n",
        "#-------------------------------------------------\n",
        "# ------------Training-----------------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "loss_fcn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "for ep in range(epochs):\n",
        "    logs = {}\n",
        "\n",
        "    # Run a training epoch, then a validation epoch\n",
        "    for phase in ['train', 'validation']:\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.\n",
        "        counter = 0.\n",
        "    \n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "        \n",
        "        for x, label in dataloaders[phase]:\n",
        "\n",
        "            if phase == 'train':\n",
        "                prediction = model(x.cuda)\n",
        "                loss = loss_fcn(prediction, label.cuda)\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                \n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    prediction = model(x.cuda)\n",
        "                    loss = loss_fcn(prediction, label.cuda)\n",
        "                    \n",
        "\n",
        "            preds = (torch.sigmoid(prediction) >= .5).float()\n",
        "            counter += float(x.shape[0])\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(preds == label).item()\n",
        "\n",
        "            \n",
        "        prefix = ''\n",
        "        if phase == 'validation':\n",
        "            prefix = 'val_'\n",
        "        \n",
        "        logs[prefix + 'loss'] = running_loss / counter\n",
        "        logs[prefix + 'acc'] = running_corrects / counter\n",
        "\n",
        "    if scheduler is not None:\n",
        "        scheduler.step()\n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    liveloss.update(logs)\n",
        "    liveloss.draw()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2a880badba9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehYC3FQDXlOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef1cecc-b3ec-4e7b-e295-5d37fac7fa91"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# ------------Test Submission---------------------\n",
        "#-------------------------------------------------\n",
        "\n",
        "team_name = 'baseline'\n",
        "if team_name == 'baseline':\n",
        "    print(\"Don't forget to change the team name for your submission.\")\n",
        "    \n",
        "test_data = load_data('data/test_data.npy')\n",
        "test_augmentation = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "dataset_test = IcebergDataset(\n",
        "    test_data,\n",
        "    augmentation=test_augmentation,\n",
        "    is_test=True\n",
        ")\n",
        "\n",
        "data_loader_test = DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=32,\n",
        "    num_workers=0,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions_ = []\n",
        "# save the model output directly to numpy file, output shape should be (604,)\n",
        "for x in data_loader_test:\n",
        "    prediction = model(x.to(device))\n",
        "    predictions_.append(prediction.detach().cpu().numpy())\n",
        "    \n",
        "predictions_ = np.concatenate(predictions_,0)\n",
        "np.save('data/' + team_name + '_submission.npy',predictions_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Don't forget to change the team name for your submission.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIZFQWtmXlOv"
      },
      "source": [
        "# Your submission file should be named [team_name]_submission.npy!\n",
        "## Provide a file `[team_name]_comment.txt` that briefly explains your approach!\n"
      ]
    }
  ]
}