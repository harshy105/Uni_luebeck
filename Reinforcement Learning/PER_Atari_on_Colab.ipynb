{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PER_Atari_on_Colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKevL7MMqhfs"
      },
      "source": [
        "##-----degraded-----##\n",
        "# !pip install -Iv gym==0.17.1\n",
        "# !apt-get install python-opengl -y\n",
        "# !apt install xvfb -y\n",
        "# !pip install gym[atari]\n",
        "# !pip install pyvirtualdisplay\n",
        "# !conda install piglet\n",
        "# !pip install pystan\n",
        "# !conda install swig\n",
        "# #!pip install box2d-py\n",
        "# !pip install box2d-py\n",
        "# !pip install gym[box2d]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBbKJBtWqwGk",
        "outputId": "62f328aa-1b03-49b1-a53c-0d6e6b3df919"
      },
      "source": [
        "# Upload 'HC ROMS.zip' and 'ROMS.zip' online.\n",
        "!python -m atari_py.import_roms .\n",
        "import gym\n",
        "env = gym.make('PongNoFrameskip-v4')\n",
        "print(env.observation_space.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "copying adventure.bin from ROMS/Adventure (1980) (Atari, Warren Robinett) (CX2613, CX2613P) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n",
            "copying air_raid.bin from ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n",
            "copying alien.bin from ROMS/Alien (1982) (20th Century Fox Video Games, Douglas 'Dallas North' Neubauer) (11006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n",
            "copying amidar.bin from ROMS/Amidar (1982) (Parker Brothers, Ed Temple) (PB5310) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n",
            "copying assault.bin from ROMS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n",
            "copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n",
            "copying asteroids.bin from ROMS/Asteroids (1981) (Atari, Brad Stewart - Sears) (CX2649 - 49-75163) [no copyright] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n",
            "copying atlantis.bin from ROMS/Atlantis (Lost City of Atlantis) (1982) (Imagic, Dennis Koble) (720103-1A, 720103-1B, IA3203, IX-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n",
            "copying bank_heist.bin from ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983) (20th Century Fox Video Games, Bill Aspromonte) (11012) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n",
            "copying battle_zone.bin from ROMS/Battlezone (1983) (Atari - GCC, Mike Feinstein, Brad Rice) (CX2681) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n",
            "copying beam_rider.bin from ROMS/Beamrider (1984) (Activision - Cheshire Engineering, David Rolfe, Larry Zwick) (AZ-037-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n",
            "copying berzerk.bin from ROMS/Berzerk (1982) (Atari, Dan Hitchens - Sears) (CX2650 - 49-75168) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n",
            "copying bowling.bin from ROMS/Bowling (1979) (Atari, Larry Kaplan - Sears) (CX2628 - 6-99842, 49-75117) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n",
            "copying boxing.bin from ROMS/Boxing - La Boxe (1980) (Activision, Bob Whitehead) (AG-002, CAG-002, AG-002-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n",
            "copying breakout.bin from ROMS/Breakout - Breakaway IV (Paddle) (1978) (Atari, Brad Stewart - Sears) (CX2622 - 6-99813, 49-75107) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n",
            "copying carnival.bin from ROMS/Carnival (1982) (Coleco - Woodside Design Associates, Steve 'Jessica Stevens' Kitchen) (2468) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n",
            "copying centipede.bin from ROMS/Centipede (1983) (Atari - GCC) (CX2676) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n",
            "copying chopper_command.bin from ROMS/Chopper Command (1982) (Activision, Bob Whitehead) (AX-015, AX-015-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n",
            "copying crazy_climber.bin from ROMS/Crazy Climber (1983) (Atari - Roklan, Joe Gaucher, Alex Leavens) (CX2683) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n",
            "copying defender.bin from ROMS/Defender (1982) (Atari, Robert C. Polaro, Alan J. Murphy - Sears) (CX2609 - 49-75186) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n",
            "copying demon_attack.bin from ROMS/Demon Attack (Death from Above) (1982) (Imagic, Rob Fulop) (720000-200, 720101-1B, 720101-1C, IA3200, IA3200C, IX-006-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n",
            "copying donkey_kong.bin from ROMS/Donkey Kong (1982) (Coleco - Woodside Design Associates - Imaginative Systems Software, Garry Kitchen) (2451) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n",
            "copying double_dunk.bin from ROMS/Double Dunk (Super Basketball) (1989) (Atari, Matthew L. Hubbard) (CX26159) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n",
            "copying elevator_action.bin from ROMS/Elevator Action (1983) (Atari, Dan Hitchens) (CX26126) (Prototype) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n",
            "copying enduro.bin from ROMS/Enduro (1983) (Activision, Larry Miller) (AX-026, AX-026-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n",
            "copying fishing_derby.bin from ROMS/Fishing Derby (1980) (Activision, David Crane) (AG-004) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n",
            "copying freeway.bin from ROMS/Freeway (1981) (Activision, David Crane) (AG-009, AG-009-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n",
            "copying frogger.bin from ROMS/Frogger (1982) (Parker Brothers, Ed English, David Lamkins) (PB5300) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n",
            "copying frostbite.bin from ROMS/Frostbite (1983) (Activision, Steve Cartwright) (AX-031) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n",
            "copying galaxian.bin from ROMS/Galaxian (1983) (Atari - GCC, Mark Ackerman, Tom Calderwood, Glenn Parker) (CX2684) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n",
            "copying gopher.bin from ROMS/Gopher (Gopher Attack) (1982) (U.S. Games Corporation - JWDA, Sylvia Day, Todd Marshall, Robin McDaniel, Henry Will IV) (VC2001) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n",
            "copying gravitar.bin from ROMS/Gravitar (1983) (Atari, Dan Hitchens, Mimi Nyden) (CX2685) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n",
            "copying hero.bin from ROMS/H.E.R.O. (1984) (Activision, John Van Ryzin) (AZ-036-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n",
            "copying ice_hockey.bin from ROMS/Ice Hockey - Le Hockey Sur Glace (1981) (Activision, Alan Miller) (AX-012, CAX-012, AX-012-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n",
            "copying jamesbond.bin from ROMS/James Bond 007 (James Bond Agent 007) (1984) (Parker Brothers - On-Time Software, Joe Gaucher, Louis Marbel) (PB5110) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n",
            "copying journey_escape.bin from ROMS/Journey Escape (1983) (Data Age, J. Ray Dettling) (112-006) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n",
            "copying kaboom.bin from ROMS/Kaboom! (Paddle) (1981) (Activision, Larry Kaplan, David Crane) (AG-010, AG-010-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n",
            "copying kangaroo.bin from ROMS/Kangaroo (1983) (Atari - GCC, Kevin Osborn) (CX2689) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n",
            "copying keystone_kapers.bin from ROMS/Keystone Kapers - Raueber und Gendarm (1983) (Activision, Garry Kitchen - Ariola) (EAX-025, EAX-025-04I - 711 025-725) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n",
            "copying king_kong.bin from ROMS/King Kong (1982) (Tigervision - Software Electronics Corporation, Karl T. Olinger - Teldec) (7-001 - 3.60001 VE) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n",
            "copying koolaid.bin from ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983) (M Network, Stephen Tatsumi, Jane Terjung - Kool Aid) (MT4648) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n",
            "copying krull.bin from ROMS/Krull (1983) (Atari, Jerome Domurat, Dave Staugas) (CX2682) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n",
            "copying kung_fu_master.bin from ROMS/Kung-Fu Master (1987) (Activision - Imagineering, Dan Kitchen, Garry Kitchen) (AG-039-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n",
            "copying laser_gates.bin from ROMS/Laser Gates (AKA Innerspace) (1983) (Imagic, Dan Oliver) (720118-2A, 13208, EIX-007-04I) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n",
            "copying lost_luggage.bin from ROMS/Lost Luggage (Airport Mayhem) (1982) (Apollo - Games by Apollo, Larry Minor, Ernie Runyon, Ed Salvo) (AP-2004) [no opening scene] ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n",
            "copying montezuma_revenge.bin from ROMS/Montezuma's Revenge - Featuring Panama Joe (1984) (Parker Brothers - JWDA, Henry Will IV) (PB5760) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n",
            "copying mr_do.bin from ROMS/Mr. Do! (1983) (CBS Electronics, Ed English) (4L4478) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n",
            "copying ms_pacman.bin from ROMS/Ms. Pac-Man (1983) (Atari - GCC, Mark Ackerman, Glenn Parker) (CX2675) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n",
            "copying name_this_game.bin from ROMS/Name This Game (Guardians of Treasure) (1983) (U.S. Games Corporation - JWDA, Roger Booth, Sylvia Day, Ron Dubren, Todd Marshall, Robin McDaniel, Wes Trager, Henry Will IV) (VC1007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n",
            "copying pacman.bin from ROMS/Pac-Man (1982) (Atari, Tod Frye) (CX2646) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n",
            "copying phoenix.bin from ROMS/Phoenix (1983) (Atari - GCC, Mike Feinstein, John Mracek) (CX2673) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n",
            "copying video_pinball.bin from ROMS/Pinball (AKA Video Pinball) (Zellers).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n",
            "copying pitfall.bin from ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982) (Activision, David Crane) (AX-018, AX-018-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n",
            "copying pooyan.bin from ROMS/Pooyan (1983) (Konami) (RC 100-X 02) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n",
            "copying private_eye.bin from ROMS/Private Eye (1984) (Activision, Bob Whitehead) (AG-034-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n",
            "copying qbert.bin from ROMS/Q-bert (1983) (Parker Brothers - Western Technologies, Dave Hampton, Tom Sloper) (PB5360) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n",
            "copying riverraid.bin from ROMS/River Raid (1982) (Activision, Carol Shaw) (AX-020, AX-020-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n",
            "copying road_runner.bin from patched version of ROMS/Road Runner (1989) (Atari - Bobco, Robert C. Polaro) (CX2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n",
            "copying robotank.bin from ROMS/Robot Tank (Robotank) (1983) (Activision, Alan Miller) (AZ-028, AG-028-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n",
            "copying seaquest.bin from ROMS/Seaquest (1983) (Activision, Steve Cartwright) (AX-022) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n",
            "copying sir_lancelot.bin from ROMS/Sir Lancelot (1983) (Xonox - K-Tel Software - Product Guild, Anthony R. Henderson) (99006, 6220) (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n",
            "copying skiing.bin from ROMS/Skiing - Le Ski (1980) (Activision, Bob Whitehead) (AG-005, CAG-005, AG-005-04) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n",
            "copying solaris.bin from ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986) (Atari, Douglas Neubauer, Mimi Nyden) (CX26136) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n",
            "copying space_invaders.bin from ROMS/Space Invaders (1980) (Atari, Richard Maurer - Sears) (CX2632 - 49-75153) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n",
            "copying star_gunner.bin from ROMS/Stargunner (1983) (Telesys, Alex Leavens) (1005) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n",
            "copying surround.bin from ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n",
            "copying tennis.bin from ROMS/Tennis - Le Tennis (1981) (Activision, Alan Miller) (AG-007, CAG-007) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n",
            "copying time_pilot.bin from ROMS/Time Pilot (1983) (Coleco - Woodside Design Associates, Harley H. Puthuff Jr.) (2663) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n",
            "copying trondead.bin from ROMS/TRON - Deadly Discs (TRON Joystick) (1983) (M Network - INTV - APh Technological Consulting, Jeff Ronne, Brett Stutz) (MT5662) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n",
            "copying tutankham.bin from ROMS/Tutankham (1983) (Parker Brothers, Dave Engman, Dawn Stockbridge) (PB5340) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n",
            "copying up_n_down.bin from ROMS/Up 'n Down (1984) (SEGA - Beck-Tech, Steve Beck, Phat Ho) (009-01) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n",
            "copying venture.bin from ROMS/Venture (1982) (Coleco, Joseph Biel) (2457) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n",
            "copying pong.bin from ROMS/Video Olympics - Pong Sports (Paddle) (1977) (Atari, Joe Decuir - Sears) (CX2621 - 99806, 6-99806, 49-75104) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n",
            "copying wizard_of_wor.bin from ROMS/Wizard of Wor (1982) (CBS Electronics - Roklan, Joe Hellesen, Joe Wagner) (M8774, M8794) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n",
            "copying yars_revenge.bin from ROMS/Yars' Revenge (Time Freeze) (1982) (Atari, Howard Scott Warshaw - Sears) (CX2655 - 49-75167) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n",
            "copying zaxxon.bin from ROMS/Zaxxon (1983) (Coleco) (2454) ~.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n",
            "(210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb1RrAgW2klq"
      },
      "source": [
        "#(i) Create a folder called 'common'\n",
        "# Then uploaad the .py file 'old_atari_wrappers.py' to 'common'\n",
        "# (ii) Copy the main code from .py file to this block and finish the code.\n",
        "# NOTE: COLAB can maximlly connect with 12 hrs, and you will need to check this tab from time to time to see if it is still connected or let you check whether you are ROBOT."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDqjVwyCHAj"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed May 12 22:46:13 2021\n",
        "This task requires some efforts and is evlauted as the highest difficulty-level so far. Not really difficult, but a lot of details.\n",
        "You can take the code from Bonus task 1 to use here.\n",
        "@author: hongh\n",
        "\"\"\"\n",
        "import math, random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from common.old_atari_wrappers import wrap_pytorch, make_atari, wrap_deepmind\n",
        "import matplotlib.pyplot as plt\n",
        "            \n",
        "\n",
        "def init_weights(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        # To do: using torch.nn.init.kaiming_normal_() to initialize m.weight , choose the proper nonlinearity \n",
        "        # m.bias can be intialized to be zero, using torch.nn.init.zeros_()\n",
        "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in',nonlinearity='relu')\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        # To do: Since we use the leakyReLU with slope of 0.2, you need to choose appropriate nonlinearity slope.\n",
        "        # bias and weights need to be intialized respectively.\n",
        "        torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', a=0.2, nonlinearity='leaky_relu')\n",
        "        torch.nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):    \n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # build your network according to the paper \"Playing Atari with Deep Reinforcement Learning\"\n",
        "        # To prevent \"dead neurons\" of ReLU activation, you could use instead Leakyrelu with the negative slope of 0.2 after the fully-connected layer with 256 neurons.       \n",
        "        # Hint: nn.sequential() , nn.Conv2d(), nn.Linear()....\n",
        "        \n",
        "        # To do: self.features only contains the 3 convolutional blocks and their nonliner activation functions\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # To do: apply the weight initialization scheme on self.features using the defined function init_weights.\n",
        "        self.features.apply(init_weights)\n",
        "\n",
        "        \n",
        "        \n",
        "        # To do: Initialize fully connected layer () , then followed by leaklyReLU with slope 0.2, (We don't use ReLU here), and last linear layer\n",
        "        self.fc1 = nn.Linear(64*7*7, 256)\n",
        "        self.fc1.apply(init_weights)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.fc2 = nn.Linear(256, num_actions)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, state):\n",
        "        # To do: before forwarding the state, the state should be normalized between [0,1], the original pixel range is [0,255]\n",
        "        state = torch.div(state, 255)\n",
        "        state = state.cuda()\n",
        "        \n",
        "        feats = self.features(state)\n",
        "        fc1 = self.fc1(feats.view(feats.size(0),-1))\n",
        "        leaky_relu = self.leaky_relu(fc1)\n",
        "\n",
        "        return self.fc2(leaky_relu).cuda()\n",
        "\n",
        "    def feature_size(self):\n",
        "        with torch.no_grad():\n",
        "            return self.features(torch.zeros(1, *self.input_shape)).view(1, -1).size(1)\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxTh-2DrCJel"
      },
      "source": [
        "\n",
        "\n",
        "class LearningAgent: \n",
        "    def __init__(self, **kwargs):      \n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(\"Using {} for training the algorithm.\".format(self.device))        \n",
        "        self.env = kwargs.get('training_env',None)\n",
        "        self.gamma = kwargs.get('gamma', 0.99)\n",
        "        self.hard_update_interval = kwargs.get('target network update interval', 10000)\n",
        "        \n",
        "        self.update_step = 0\n",
        "        \n",
        "        # ---- initialize networks ----\n",
        "        self.q_net1 = QNetwork(env.observation_space.shape, env.action_space.n).to(self.device)\n",
        "        self.target_q_net1 = QNetwork(env.observation_space.shape, env.action_space.n).to(self.device)\n",
        "\n",
        "\n",
        "        # To do: initialize optimizers using Adam optimizer, learning rate is in kwargs.\n",
        "        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), kwargs.get('learning rate', 1e-4))\n",
        "        # To do: initialize the loss function (Huberloss) , keywords:  torch.nn.SmoothL1Loss\n",
        "        # Note : here the reduction scheme should be set to be 'none', different from previous one. It is needed for incorporating IS-weights.\n",
        "        self.criterion = nn.SmoothL1Loss(reduction='none')\n",
        "        \n",
        "        # IMPORTANT: In our implementation, we use sum tree, whose root node number must be 2^n. \n",
        "        self.replay_buffer = PER(kwargs.get('memory size',1048576), self.env.observation_space.shape)\n",
        "        \n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        # To do: implement epsilon-greedy, where action is the index (just a number) , not a array/list/tensor.\n",
        "        # Hint: to speed up, use with torch.no_grad():\n",
        "        state = torch.tensor(state)\n",
        "        if state.size(0) == 4:\n",
        "          state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          # Epsilon-Greedy action selection\n",
        "          if np.random.random() > self.epsilon: # Exploit\n",
        "            action = self.q_net1.forward(state)\n",
        "            act = torch.argmax(action).item()\n",
        "          else:  # Explore\n",
        "            act = env.action_space.sample()\n",
        "        return act\n",
        "\n",
        "\n",
        "    def test_act(self, state):\n",
        "        # To do: implement greedy policy, where action is the index (just a number) , not a array/list/tensor.\n",
        "        # Hint: to speed up, use with torch.no_grad():\n",
        "        state = torch.tensor(state)\n",
        "        if state.size(0) == 4:\n",
        "          state = state.unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "          action = self.q_net1.forward(state)\n",
        "          act = torch.argmax(action).item()\n",
        "        return act\n",
        "    \n",
        "    \n",
        "   \n",
        "    def update(self, batch_size):       \n",
        "        # To do: implement DDQN with prioritized experience replay, now the loss is different due to IS weights.\n",
        "        self.q_net1.train()\n",
        "        \n",
        "        transitions, idxs, weights = self.replay_buffer.sample(batch_size)        \n",
        "        states, actions, rewards, next_states, dones = transitions\n",
        "        weights = torch.FloatTensor(weights).to(self.device)\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "        \n",
        "        #DDQN.....\n",
        "        with torch.no_grad():\n",
        "          #DDQN\n",
        "          act_idx = self.q_net1(next_states).argmax(1).unsqueeze(1)\n",
        "          next_state_act_vals = self.target_q_net1(next_states).gather(1, act_idx)\n",
        "          y_i = (1 - dones)*(next_state_act_vals.squeeze(1) * self.gamma) + rewards#DDQN\n",
        "\n",
        "        state_action_values = self.q_net1(states).gather(1, actions.unsqueeze(1)) #self.q_net1(states).gather(1, act_idx)\n",
        "\n",
        "\n",
        "        # To do:  q loss using self.criterion\n",
        "        q1_loss = self.criterion(state_action_values, y_i.detach().unsqueeze(1)).squeeze()     \n",
        "          \n",
        "        # For PER algorithm code line 12,  To do: convert q1_loss from (Cuda) tensor to numpy, \n",
        "        td_error_abs = np.abs(q1_loss.detach().cpu().numpy()) + 1e-7#rewards.detach().cpu().numpy() + self.gamma*q1_loss.detach().cpu().numpy() + 1e-7 # adding 1e-7 to avoid singulrities.\n",
        "        # To do: incorporate IS_weight for each sample and then taking the mean.\n",
        "        #q1_loss = np.mean((state_action_values, y_i.detach().unsqueeze(1)) * weights)#np.mean(np.sqrt((state_action_values.detach().cpu().numpy() - y_i.unsqueeze(1).detach().cpu().numpy()) ** 2) * weights.detach().cpu().numpy())#...\n",
        "        \n",
        "        \n",
        "        # update priority for trained samples\n",
        "        for idx, td_error in zip(idxs, td_error_abs):\n",
        "            self.replay_buffer.update_priority(idx, td_error)\n",
        "\n",
        "        q1_loss = (q1_loss*weights).mean()\n",
        "        # update q networks        \n",
        "        self.q1_optimizer.zero_grad()\n",
        "        q1_loss.backward()\n",
        "        self.q1_optimizer.step()\n",
        "\n",
        "        # --------To do: Perform Hard update of target networks every self.hard_update_interval-----------     \n",
        "        if self.update_step % self.hard_update_interval == 0:    \n",
        "          for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "        \n",
        "        self.update_step += 1\n",
        "        \n",
        "        # just for monitoring Q\n",
        "        if self.update_step % 1000 == 999:\n",
        "            print(\"Q loss: {}\".format(q1_loss))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASPVJeAdCLM9"
      },
      "source": [
        "\n",
        "\n",
        "class SumTree:\n",
        "    \n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.data = np.zeros(capacity, dtype=object) # storing experience\n",
        "        self.tree = np.zeros(2 * capacity - 1) # storing pripority + parental nodes. If you have n elements in the bottom, then you need n + (n-1) nodes to construct a tree.        \n",
        "        self.n_entries = 0\n",
        "        self.overwrite_start_flag = False # record whether N_entry > capacity\n",
        "        \n",
        "    # update to the root node\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    # find sample on leaf node\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    # store priority and sample\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1 # starting write from the first element of the bottom layer,\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "            self.overwrite_start_flag = True\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "\n",
        "    # update priority\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    # get priority and sample\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])   \n",
        "    \n",
        "    def max_priority(self):\n",
        "        return np.max(self.tree[self.capacity-1:])\n",
        "\n",
        "    # ---- modified : to avoid min_prob being overwritten, causing a potential change in max_wi --> Q-loss suddenly changes/rescales --> unstable training  ----\n",
        "    def min_prob(self):\n",
        "        ''' To do: \n",
        "            return: p_min, the minimal of P(j) in the replay buffer, see line 9 in algorithm in the original paper https://arxiv.org/pdf/1511.05952.pdf.\n",
        "            p_min is used to determine max w_{i}\n",
        "        '''\n",
        "        #print(self.tree)\n",
        "        \n",
        "        #nonzero_idxs = np.where(self.tree != 0.0)\n",
        "        #values = self.tree[nonzero_idxs]\n",
        "        p = self.tree[self.capacity-1:]\n",
        "\n",
        "        if self.overwrite_start_flag == False:        \n",
        "            p_min = np.min(p[p > 0])#np.min(self.tree[self.capacity-1:])\n",
        "            self.p_min_history = p_min\n",
        "        elif self.overwrite_start_flag == True: \n",
        "            p_min = min(np.min(p[p > 0]),self.p_min_history)\n",
        "            self.p_min_history = min(p_min, self.p_min_history)\n",
        "        return p_min\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0edunNiCRqj"
      },
      "source": [
        "\n",
        "class PER(object):\n",
        "    '''Here we only use  proportional prioritization , not rank-based prioritiation, which is reported to be of better performance.\n",
        "    '''\n",
        "    def __init__(self, max_size, shape, alpha=0.6, beta=0.4):\n",
        "        self.sum_tree = SumTree(max_size)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.beta_increment = (1.0 - beta)/100000.\n",
        "        self.current_length = 0\n",
        "        self.frame_stack = shape[0]\n",
        "        self.state_dim = shape\n",
        "        \n",
        "        \n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        # To do: store the priority\n",
        "        if self.current_length == 0: # The first sample with priority = 1\n",
        "            priority = 1.0  \n",
        "        else: # maximal priority\n",
        "            priority = self.sum_tree.max_priority()\n",
        "        self.current_length = self.current_length + 1\n",
        "        # To Note: we directly priority = td_error ^ self.alpha into the node, which is befneficial for sampling later.\n",
        "       \n",
        "\n",
        "        experience = (state, np.array([action]), np.array([reward]), next_state, done)\n",
        "        self.sum_tree.add(priority, experience)\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Perform a Stochastic universal sampling by roulette wheel, which reduces the variance.\n",
        "        batch_idx, batch, priorities = [], [], []\n",
        "        segment = self.sum_tree.total() / batch_size\n",
        "        p_sum = self.sum_tree.tree[0]\n",
        "    \n",
        "        for i in range(batch_size): \n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = random.uniform(a, b)\n",
        "            idx, p, data = self.sum_tree.get(s)\n",
        "            batch_idx.append(idx)\n",
        "            batch.append(data)\n",
        "            priorities.append(p)\n",
        "            \n",
        "        # To do: Compute the IS_weights for each sampled experience.           \n",
        "        prob = priorities / p_sum # a 1-D array with the length of batch size\n",
        "        IS_weights = np.power((self.current_length * prob), -self.beta) # a function of prob\n",
        "        max_weight = np.power((1/self.sum_tree.n_entries) * (1/(self.sum_tree.min_prob()/p_sum)), self.beta) # Hint : you will need to call self.sum_tree.min_prob()\n",
        "        IS_weights /= max_weight # should be of the same shape as 'prob'\n",
        "               \n",
        "        state_batch = np.empty((batch_size,*self.state_dim))\n",
        "        action_batch = np.empty((batch_size))\n",
        "        reward_batch = np.empty((batch_size))\n",
        "        next_state_batch = np.empty((batch_size,*self.state_dim))\n",
        "        done_batch = np.empty((batch_size))   \n",
        "\n",
        "        for i,transition in enumerate(batch):\n",
        "            state, action, reward, next_state, done = transition\n",
        "            state_batch[i] = state\n",
        "            action_batch[i] = action\n",
        "            reward_batch[i] = reward\n",
        "            next_state_batch[i] = next_state\n",
        "            done_batch[i] = done\n",
        "\n",
        "        # To do: update beta.\n",
        "        self.beta = min(1, self.beta + self.beta_increment)\n",
        "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch), batch_idx, IS_weights\n",
        "\n",
        "    def update_priority(self, idx, td_error):\n",
        "        # Slightly different from original code, we save the exponeniated priority as follows to the sum tree, which is easier to take samples according to the probability later.\n",
        "        # To do: priority = td_error ^ self.alpha\n",
        "        priority = np.power(td_error, self.alpha)\n",
        "        self.sum_tree.update(idx, priority)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.current_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0aruDl1Xuvu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e18bf81-d5e7-4117-e458-d5a409e3be2f"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def learn(agent, **kwargs):   \n",
        "    update_step = 0\n",
        "    train_start = kwargs.get('number of pre-interactions to start training', 50000)\n",
        "    batch_size = kwargs.get('batch size', 32)\n",
        "    try:\n",
        "        max_steps = env._max_episode_steps# \n",
        "    except:\n",
        "        max_steps = kwargs.get('max steps per episode', 27500)   \n",
        "    max_episodes = kwargs.get('max episodes', 1000000)   \n",
        "    update_every_n_step = kwargs.get('update every n steps', 4)   \n",
        "    n_frame = 0\n",
        "    running_100_mean = np.zeros(100)    \n",
        "    epsilon_start = 1\n",
        "    epsilon_final = 0.1\n",
        "#    epsilon_decay = 10000\n",
        "    \n",
        "    epi_returns = [] # Important: you will save this statistics for plotting\n",
        "    test_phase_avg_returns = []\n",
        "\n",
        "    for episode in range(max_episodes):    \n",
        "                \n",
        "        state = env.reset()\n",
        "        epi_return = 0\n",
        "        for step in itertools.count():\n",
        "            # Implement linearly-decaying epsilon\n",
        "            agent.epsilon = max(epsilon_final, epsilon_start - (epsilon_start - epsilon_final)* n_frame/500000. )\n",
        "            action= agent.act(state)                            \n",
        "            next_state, reward, done, _ = env.step(action)            \n",
        "            n_frame+=1        \n",
        "            # Push the experience into replay buffer                                                              \n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            epi_return += reward\n",
        "\n",
        "            # To do: write an if condition that the training starts after the agent collects enough experience () and performs an update every 4 interactions.\n",
        "            # Useful variable :  train_start , update_every_n_step , n_frame\n",
        "            if (len(agent.replay_buffer) > train_start) and ((n_frame % update_every_n_step) == 0): \n",
        "                agent.update(batch_size)\n",
        "                update_step += 1\n",
        "            state = next_state\n",
        "            \n",
        "            if done == True or step >= max_steps:\n",
        "                running_100_mean[episode%100] = epi_return\n",
        "                epi_returns.append(epi_return)\n",
        "                print('Episode : {} , episodic length : {}, return: {}, exp_count :{}, averged returnover past 100 episodes : {}'.format(episode , step, epi_return, n_frame, np.mean(running_100_mean[:min(100,episode+1)]))) \n",
        "                break\n",
        "\n",
        "        #---------------To Do: Save the satistics of ----------------\n",
        "        if episode%50 == 49: # you can change here\n",
        "            epi_returns_np = np.array(epi_returns)\n",
        "            #To do: save the statistics here for plotting \n",
        "            np.save(\"epi_returns_PER.npy\", epi_returns_np)\n",
        "\n",
        "        #----------------------------Testing-----------------------\n",
        "        if episode%100 == 99:\n",
        "            test_returns = np.zeros(15)\n",
        "            print ('-------start Testing--------')\n",
        "            epi_return = 0       \n",
        "            state = test_env.reset()  \n",
        "            for i in range(15): # test for 15 episodes\n",
        "                state = test_env.reset() \n",
        "                for t in range(max_steps):\n",
        "                    # env.render()\n",
        "                    action = agent.test_act(state)\n",
        "                    next_state, reward, done, _ = test_env.step(action)\n",
        "                    state = next_state\n",
        "                    epi_return += reward                        \n",
        "                    if done or (t == max_steps - 1):\n",
        "                        test_returns[i] = epi_return\n",
        "                        epi_return = 0\n",
        "                        break         \n",
        "            test_phase_avg_returns.append(np.mean(test_returns))           \n",
        "            print('Testing result, Returns: {}'.format(test_returns))\n",
        "            test_phase_avg_returns_np = np.array(test_phase_avg_returns)\n",
        "            # ---IMPORTNT: To do: save the statistics here for plotting ---\n",
        "            np.save(\"test_phase_avg_returns.npy\", test_phase_avg_returns_np)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':  \n",
        "    \n",
        "    env_id = \"PongNoFrameskip-v4\" # you could change to other games, but only use 'NoFrameskip-v4' version. Some games e.g., spaceinvaders require changing the code in wrapper, i.e., frame_skip =3\n",
        "    env = make_atari(env_id)\n",
        "    env = wrap_deepmind(env)\n",
        "    env = wrap_pytorch(env) # Note the env returns the unnormalized pixel [0,255].\n",
        "    test_env = make_atari(env_id)\n",
        "    test_env = wrap_deepmind(test_env, episode_life=False, clip_rewards=False)\n",
        "    test_env = wrap_pytorch(test_env)#wrap_pytorch_with_normalization(env)\n",
        "    \n",
        "    # Hint: you could check the shape and content of the state, it is not yet normlized between [0,1]\n",
        "    obs = env.reset()\n",
        "    print(obs.shape)\n",
        "    for i in range(obs.shape[0]):\n",
        "        plt.imshow(obs[i])\n",
        "    \n",
        "    random_seed = None\n",
        "    if random_seed:\n",
        "        print(\"Random Seed: {}\".format(random_seed))\n",
        "        torch.manual_seed(random_seed)\n",
        "        env.seed(random_seed)\n",
        "        np.random.seed(random_seed)  \n",
        "        random.seed(random_seed)\n",
        "        \n",
        "    # DQN params\n",
        "    config = {}\n",
        "    config.update({'gamma': 0.99})\n",
        "    config.update({'learning rate': 1e-4})\n",
        "    config.update({'memory size': 262144}) # If you use Prioritized EXP, size must be 2^n. 1M frame memory buffer takes around 8GB in RAM. For local computer, you could use higher memory size.\n",
        "    config.update({'target network update interval': 3000}) # every 3000 training steps (not interaction steps), update the target model.\n",
        "    config.update({'max episodes': 1000000}) # you don't need to run that long, but just keep running.\n",
        "    config.update({'max steps per episode': 27500}) \n",
        "    config.update({'batch size': 32}) \n",
        "    config.update({'update every n steps': 4})\n",
        "    config.update({'number of pre-interactions to start training': 50000})\n",
        "    config.update({'training_env': env})\n",
        "    config.update({'testing_env': test_env})\n",
        "    #  agent\n",
        "    agent = LearningAgent(**config)\n",
        "    \n",
        "    # train\n",
        "    learn(agent, **config)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 84, 84)\n",
            "Using cuda for training the algorithm.\n",
            "Episode : 0 , episodic length : 848, return: -20.0, exp_count :849, averged returnover past 100 episodes : -20.0\n",
            "Episode : 1 , episodic length : 1137, return: -18.0, exp_count :1987, averged returnover past 100 episodes : -19.0\n",
            "Episode : 2 , episodic length : 1015, return: -19.0, exp_count :3003, averged returnover past 100 episodes : -19.0\n",
            "Episode : 3 , episodic length : 936, return: -21.0, exp_count :3940, averged returnover past 100 episodes : -19.5\n",
            "Episode : 4 , episodic length : 880, return: -20.0, exp_count :4821, averged returnover past 100 episodes : -19.6\n",
            "Episode : 5 , episodic length : 900, return: -21.0, exp_count :5722, averged returnover past 100 episodes : -19.833333333333332\n",
            "Episode : 6 , episodic length : 894, return: -21.0, exp_count :6617, averged returnover past 100 episodes : -20.0\n",
            "Episode : 7 , episodic length : 1062, return: -20.0, exp_count :7680, averged returnover past 100 episodes : -20.0\n",
            "Episode : 8 , episodic length : 760, return: -21.0, exp_count :8441, averged returnover past 100 episodes : -20.11111111111111\n",
            "Episode : 9 , episodic length : 1005, return: -20.0, exp_count :9447, averged returnover past 100 episodes : -20.1\n",
            "Episode : 10 , episodic length : 891, return: -21.0, exp_count :10339, averged returnover past 100 episodes : -20.181818181818183\n",
            "Episode : 11 , episodic length : 899, return: -21.0, exp_count :11239, averged returnover past 100 episodes : -20.25\n",
            "Episode : 12 , episodic length : 848, return: -21.0, exp_count :12088, averged returnover past 100 episodes : -20.307692307692307\n",
            "Episode : 13 , episodic length : 882, return: -21.0, exp_count :12971, averged returnover past 100 episodes : -20.357142857142858\n",
            "Episode : 14 , episodic length : 831, return: -20.0, exp_count :13803, averged returnover past 100 episodes : -20.333333333333332\n",
            "Episode : 15 , episodic length : 835, return: -21.0, exp_count :14639, averged returnover past 100 episodes : -20.375\n",
            "Episode : 16 , episodic length : 1008, return: -19.0, exp_count :15648, averged returnover past 100 episodes : -20.294117647058822\n",
            "Episode : 17 , episodic length : 834, return: -21.0, exp_count :16483, averged returnover past 100 episodes : -20.333333333333332\n",
            "Episode : 18 , episodic length : 848, return: -21.0, exp_count :17332, averged returnover past 100 episodes : -20.36842105263158\n",
            "Episode : 19 , episodic length : 974, return: -20.0, exp_count :18307, averged returnover past 100 episodes : -20.35\n",
            "Episode : 20 , episodic length : 755, return: -21.0, exp_count :19063, averged returnover past 100 episodes : -20.38095238095238\n",
            "Episode : 21 , episodic length : 760, return: -21.0, exp_count :19824, averged returnover past 100 episodes : -20.40909090909091\n",
            "Episode : 22 , episodic length : 756, return: -21.0, exp_count :20581, averged returnover past 100 episodes : -20.434782608695652\n",
            "Episode : 23 , episodic length : 754, return: -21.0, exp_count :21336, averged returnover past 100 episodes : -20.458333333333332\n",
            "Episode : 24 , episodic length : 834, return: -20.0, exp_count :22171, averged returnover past 100 episodes : -20.44\n",
            "Episode : 25 , episodic length : 985, return: -20.0, exp_count :23157, averged returnover past 100 episodes : -20.423076923076923\n",
            "Episode : 26 , episodic length : 978, return: -20.0, exp_count :24136, averged returnover past 100 episodes : -20.40740740740741\n",
            "Episode : 27 , episodic length : 1194, return: -18.0, exp_count :25331, averged returnover past 100 episodes : -20.321428571428573\n",
            "Episode : 28 , episodic length : 761, return: -21.0, exp_count :26093, averged returnover past 100 episodes : -20.344827586206897\n",
            "Episode : 29 , episodic length : 898, return: -20.0, exp_count :26992, averged returnover past 100 episodes : -20.333333333333332\n",
            "Episode : 30 , episodic length : 875, return: -21.0, exp_count :27868, averged returnover past 100 episodes : -20.35483870967742\n",
            "Episode : 31 , episodic length : 761, return: -21.0, exp_count :28630, averged returnover past 100 episodes : -20.375\n",
            "Episode : 32 , episodic length : 760, return: -21.0, exp_count :29391, averged returnover past 100 episodes : -20.393939393939394\n",
            "Episode : 33 , episodic length : 881, return: -20.0, exp_count :30273, averged returnover past 100 episodes : -20.38235294117647\n",
            "Episode : 34 , episodic length : 820, return: -21.0, exp_count :31094, averged returnover past 100 episodes : -20.4\n",
            "Episode : 35 , episodic length : 1182, return: -17.0, exp_count :32277, averged returnover past 100 episodes : -20.305555555555557\n",
            "Episode : 36 , episodic length : 881, return: -21.0, exp_count :33159, averged returnover past 100 episodes : -20.324324324324323\n",
            "Episode : 37 , episodic length : 954, return: -21.0, exp_count :34114, averged returnover past 100 episodes : -20.342105263157894\n",
            "Episode : 38 , episodic length : 757, return: -21.0, exp_count :34872, averged returnover past 100 episodes : -20.358974358974358\n",
            "Episode : 39 , episodic length : 844, return: -21.0, exp_count :35717, averged returnover past 100 episodes : -20.375\n",
            "Episode : 40 , episodic length : 761, return: -21.0, exp_count :36479, averged returnover past 100 episodes : -20.390243902439025\n",
            "Episode : 41 , episodic length : 788, return: -21.0, exp_count :37268, averged returnover past 100 episodes : -20.404761904761905\n",
            "Episode : 42 , episodic length : 927, return: -21.0, exp_count :38196, averged returnover past 100 episodes : -20.41860465116279\n",
            "Episode : 43 , episodic length : 817, return: -21.0, exp_count :39014, averged returnover past 100 episodes : -20.431818181818183\n",
            "Episode : 44 , episodic length : 848, return: -21.0, exp_count :39863, averged returnover past 100 episodes : -20.444444444444443\n",
            "Episode : 45 , episodic length : 894, return: -20.0, exp_count :40758, averged returnover past 100 episodes : -20.434782608695652\n",
            "Episode : 46 , episodic length : 941, return: -21.0, exp_count :41700, averged returnover past 100 episodes : -20.4468085106383\n",
            "Episode : 47 , episodic length : 756, return: -21.0, exp_count :42457, averged returnover past 100 episodes : -20.458333333333332\n",
            "Episode : 48 , episodic length : 913, return: -19.0, exp_count :43371, averged returnover past 100 episodes : -20.428571428571427\n",
            "Episode : 49 , episodic length : 756, return: -21.0, exp_count :44128, averged returnover past 100 episodes : -20.44\n",
            "Episode : 50 , episodic length : 1317, return: -16.0, exp_count :45446, averged returnover past 100 episodes : -20.352941176470587\n",
            "Episode : 51 , episodic length : 756, return: -21.0, exp_count :46203, averged returnover past 100 episodes : -20.365384615384617\n",
            "Episode : 52 , episodic length : 760, return: -21.0, exp_count :46964, averged returnover past 100 episodes : -20.37735849056604\n",
            "Episode : 53 , episodic length : 757, return: -21.0, exp_count :47722, averged returnover past 100 episodes : -20.38888888888889\n",
            "Episode : 54 , episodic length : 822, return: -21.0, exp_count :48545, averged returnover past 100 episodes : -20.4\n",
            "Episode : 55 , episodic length : 903, return: -21.0, exp_count :49449, averged returnover past 100 episodes : -20.410714285714285\n",
            "Episode : 56 , episodic length : 862, return: -20.0, exp_count :50312, averged returnover past 100 episodes : -20.403508771929825\n",
            "Episode : 57 , episodic length : 857, return: -20.0, exp_count :51170, averged returnover past 100 episodes : -20.396551724137932\n",
            "Episode : 58 , episodic length : 1028, return: -18.0, exp_count :52199, averged returnover past 100 episodes : -20.35593220338983\n",
            "Episode : 59 , episodic length : 989, return: -19.0, exp_count :53189, averged returnover past 100 episodes : -20.333333333333332\n",
            "Q loss: 0.00017892755568027496\n",
            "Episode : 60 , episodic length : 912, return: -20.0, exp_count :54102, averged returnover past 100 episodes : -20.327868852459016\n",
            "Episode : 61 , episodic length : 834, return: -21.0, exp_count :54937, averged returnover past 100 episodes : -20.338709677419356\n",
            "Episode : 62 , episodic length : 893, return: -20.0, exp_count :55831, averged returnover past 100 episodes : -20.333333333333332\n",
            "Episode : 63 , episodic length : 816, return: -21.0, exp_count :56648, averged returnover past 100 episodes : -20.34375\n",
            "Episode : 64 , episodic length : 975, return: -20.0, exp_count :57624, averged returnover past 100 episodes : -20.338461538461537\n",
            "Q loss: 0.0027064336463809013\n",
            "Episode : 65 , episodic length : 816, return: -21.0, exp_count :58441, averged returnover past 100 episodes : -20.348484848484848\n",
            "Episode : 66 , episodic length : 782, return: -21.0, exp_count :59224, averged returnover past 100 episodes : -20.35820895522388\n",
            "Episode : 67 , episodic length : 952, return: -20.0, exp_count :60177, averged returnover past 100 episodes : -20.352941176470587\n",
            "Episode : 68 , episodic length : 939, return: -19.0, exp_count :61117, averged returnover past 100 episodes : -20.333333333333332\n",
            "Episode : 69 , episodic length : 806, return: -21.0, exp_count :61924, averged returnover past 100 episodes : -20.34285714285714\n",
            "Q loss: 0.002155120950192213\n",
            "Episode : 70 , episodic length : 891, return: -21.0, exp_count :62816, averged returnover past 100 episodes : -20.35211267605634\n",
            "Episode : 71 , episodic length : 779, return: -21.0, exp_count :63596, averged returnover past 100 episodes : -20.36111111111111\n",
            "Episode : 72 , episodic length : 868, return: -21.0, exp_count :64465, averged returnover past 100 episodes : -20.36986301369863\n",
            "Episode : 73 , episodic length : 948, return: -20.0, exp_count :65414, averged returnover past 100 episodes : -20.364864864864863\n",
            "Q loss: 0.001867875223979354\n",
            "Episode : 74 , episodic length : 757, return: -21.0, exp_count :66172, averged returnover past 100 episodes : -20.373333333333335\n",
            "Episode : 75 , episodic length : 761, return: -21.0, exp_count :66934, averged returnover past 100 episodes : -20.38157894736842\n",
            "Episode : 76 , episodic length : 1046, return: -19.0, exp_count :67981, averged returnover past 100 episodes : -20.363636363636363\n",
            "Episode : 77 , episodic length : 1061, return: -20.0, exp_count :69043, averged returnover past 100 episodes : -20.358974358974358\n",
            "Episode : 78 , episodic length : 843, return: -21.0, exp_count :69887, averged returnover past 100 episodes : -20.367088607594937\n",
            "Q loss: 0.0006909103249199688\n",
            "Episode : 79 , episodic length : 836, return: -21.0, exp_count :70724, averged returnover past 100 episodes : -20.375\n",
            "Episode : 80 , episodic length : 836, return: -20.0, exp_count :71561, averged returnover past 100 episodes : -20.37037037037037\n",
            "Episode : 81 , episodic length : 911, return: -20.0, exp_count :72473, averged returnover past 100 episodes : -20.365853658536587\n",
            "Episode : 82 , episodic length : 786, return: -21.0, exp_count :73260, averged returnover past 100 episodes : -20.373493975903614\n",
            "Q loss: 0.00016907532699406147\n",
            "Episode : 83 , episodic length : 937, return: -21.0, exp_count :74198, averged returnover past 100 episodes : -20.38095238095238\n",
            "Episode : 84 , episodic length : 848, return: -21.0, exp_count :75047, averged returnover past 100 episodes : -20.388235294117646\n",
            "Episode : 85 , episodic length : 911, return: -20.0, exp_count :75959, averged returnover past 100 episodes : -20.38372093023256\n",
            "Episode : 86 , episodic length : 850, return: -21.0, exp_count :76810, averged returnover past 100 episodes : -20.39080459770115\n",
            "Episode : 87 , episodic length : 997, return: -19.0, exp_count :77808, averged returnover past 100 episodes : -20.375\n",
            "Q loss: 0.00013907761604059488\n",
            "Episode : 88 , episodic length : 1136, return: -20.0, exp_count :78945, averged returnover past 100 episodes : -20.370786516853933\n",
            "Episode : 89 , episodic length : 1137, return: -19.0, exp_count :80083, averged returnover past 100 episodes : -20.355555555555554\n",
            "Episode : 90 , episodic length : 954, return: -20.0, exp_count :81038, averged returnover past 100 episodes : -20.35164835164835\n",
            "Q loss: 0.00022395382984541357\n",
            "Episode : 91 , episodic length : 1140, return: -19.0, exp_count :82179, averged returnover past 100 episodes : -20.33695652173913\n",
            "Episode : 92 , episodic length : 855, return: -21.0, exp_count :83035, averged returnover past 100 episodes : -20.344086021505376\n",
            "Episode : 93 , episodic length : 891, return: -20.0, exp_count :83927, averged returnover past 100 episodes : -20.340425531914892\n",
            "Episode : 94 , episodic length : 896, return: -20.0, exp_count :84824, averged returnover past 100 episodes : -20.33684210526316\n",
            "Episode : 95 , episodic length : 940, return: -21.0, exp_count :85765, averged returnover past 100 episodes : -20.34375\n",
            "Q loss: 0.00030259310733526945\n",
            "Episode : 96 , episodic length : 928, return: -21.0, exp_count :86694, averged returnover past 100 episodes : -20.350515463917525\n",
            "Episode : 97 , episodic length : 838, return: -20.0, exp_count :87533, averged returnover past 100 episodes : -20.346938775510203\n",
            "Episode : 98 , episodic length : 1063, return: -18.0, exp_count :88597, averged returnover past 100 episodes : -20.32323232323232\n",
            "Episode : 99 , episodic length : 874, return: -20.0, exp_count :89472, averged returnover past 100 episodes : -20.32\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-21. -21. -21. -21. -20. -21. -21. -21. -21. -21. -21. -21. -20. -21.\n",
            " -21.]\n",
            "Q loss: 0.0003649614518508315\n",
            "Episode : 100 , episodic length : 900, return: -21.0, exp_count :90373, averged returnover past 100 episodes : -20.33\n",
            "Episode : 101 , episodic length : 994, return: -19.0, exp_count :91368, averged returnover past 100 episodes : -20.34\n",
            "Episode : 102 , episodic length : 950, return: -21.0, exp_count :92319, averged returnover past 100 episodes : -20.36\n",
            "Episode : 103 , episodic length : 915, return: -19.0, exp_count :93235, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 0.00023158625117503107\n",
            "Episode : 104 , episodic length : 816, return: -21.0, exp_count :94052, averged returnover past 100 episodes : -20.35\n",
            "Episode : 105 , episodic length : 865, return: -21.0, exp_count :94918, averged returnover past 100 episodes : -20.35\n",
            "Episode : 106 , episodic length : 926, return: -20.0, exp_count :95845, averged returnover past 100 episodes : -20.34\n",
            "Episode : 107 , episodic length : 774, return: -21.0, exp_count :96620, averged returnover past 100 episodes : -20.35\n",
            "Episode : 108 , episodic length : 849, return: -21.0, exp_count :97470, averged returnover past 100 episodes : -20.35\n",
            "Q loss: 0.00016021353076212108\n",
            "Episode : 109 , episodic length : 830, return: -20.0, exp_count :98301, averged returnover past 100 episodes : -20.35\n",
            "Episode : 110 , episodic length : 914, return: -20.0, exp_count :99216, averged returnover past 100 episodes : -20.34\n",
            "Episode : 111 , episodic length : 761, return: -21.0, exp_count :99978, averged returnover past 100 episodes : -20.34\n",
            "Episode : 112 , episodic length : 760, return: -21.0, exp_count :100739, averged returnover past 100 episodes : -20.34\n",
            "Episode : 113 , episodic length : 1035, return: -20.0, exp_count :101775, averged returnover past 100 episodes : -20.33\n",
            "Q loss: 0.00013560912339016795\n",
            "Episode : 114 , episodic length : 814, return: -21.0, exp_count :102590, averged returnover past 100 episodes : -20.34\n",
            "Episode : 115 , episodic length : 819, return: -21.0, exp_count :103410, averged returnover past 100 episodes : -20.34\n",
            "Episode : 116 , episodic length : 990, return: -19.0, exp_count :104401, averged returnover past 100 episodes : -20.34\n",
            "Episode : 117 , episodic length : 784, return: -21.0, exp_count :105186, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 0.00013593383482657373\n",
            "Episode : 118 , episodic length : 850, return: -21.0, exp_count :106037, averged returnover past 100 episodes : -20.34\n",
            "Episode : 119 , episodic length : 922, return: -21.0, exp_count :106960, averged returnover past 100 episodes : -20.35\n",
            "Episode : 120 , episodic length : 855, return: -20.0, exp_count :107816, averged returnover past 100 episodes : -20.34\n",
            "Episode : 121 , episodic length : 760, return: -21.0, exp_count :108577, averged returnover past 100 episodes : -20.34\n",
            "Episode : 122 , episodic length : 911, return: -19.0, exp_count :109489, averged returnover past 100 episodes : -20.32\n",
            "Q loss: 0.00019873867859132588\n",
            "Episode : 123 , episodic length : 802, return: -21.0, exp_count :110292, averged returnover past 100 episodes : -20.32\n",
            "Episode : 124 , episodic length : 1156, return: -19.0, exp_count :111449, averged returnover past 100 episodes : -20.31\n",
            "Episode : 125 , episodic length : 848, return: -21.0, exp_count :112298, averged returnover past 100 episodes : -20.32\n",
            "Episode : 126 , episodic length : 990, return: -21.0, exp_count :113289, averged returnover past 100 episodes : -20.33\n",
            "Q loss: 0.00022163294488564134\n",
            "Episode : 127 , episodic length : 843, return: -21.0, exp_count :114133, averged returnover past 100 episodes : -20.36\n",
            "Episode : 128 , episodic length : 758, return: -21.0, exp_count :114892, averged returnover past 100 episodes : -20.36\n",
            "Episode : 129 , episodic length : 819, return: -21.0, exp_count :115712, averged returnover past 100 episodes : -20.37\n",
            "Episode : 130 , episodic length : 862, return: -20.0, exp_count :116575, averged returnover past 100 episodes : -20.36\n",
            "Episode : 131 , episodic length : 895, return: -20.0, exp_count :117471, averged returnover past 100 episodes : -20.35\n",
            "Q loss: 0.00036224318319000304\n",
            "Episode : 132 , episodic length : 1024, return: -19.0, exp_count :118496, averged returnover past 100 episodes : -20.33\n",
            "Episode : 133 , episodic length : 860, return: -20.0, exp_count :119357, averged returnover past 100 episodes : -20.33\n",
            "Episode : 134 , episodic length : 878, return: -21.0, exp_count :120236, averged returnover past 100 episodes : -20.33\n",
            "Episode : 135 , episodic length : 1225, return: -19.0, exp_count :121462, averged returnover past 100 episodes : -20.35\n",
            "Q loss: 0.00019999437790829688\n",
            "Episode : 136 , episodic length : 916, return: -20.0, exp_count :122379, averged returnover past 100 episodes : -20.34\n",
            "Episode : 137 , episodic length : 1004, return: -21.0, exp_count :123384, averged returnover past 100 episodes : -20.34\n",
            "Episode : 138 , episodic length : 778, return: -21.0, exp_count :124163, averged returnover past 100 episodes : -20.34\n",
            "Episode : 139 , episodic length : 893, return: -21.0, exp_count :125057, averged returnover past 100 episodes : -20.34\n",
            "Episode : 140 , episodic length : 846, return: -21.0, exp_count :125904, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 0.00019865960348397493\n",
            "Episode : 141 , episodic length : 888, return: -20.0, exp_count :126793, averged returnover past 100 episodes : -20.33\n",
            "Episode : 142 , episodic length : 778, return: -21.0, exp_count :127572, averged returnover past 100 episodes : -20.33\n",
            "Episode : 143 , episodic length : 967, return: -19.0, exp_count :128540, averged returnover past 100 episodes : -20.31\n",
            "Episode : 144 , episodic length : 1031, return: -20.0, exp_count :129572, averged returnover past 100 episodes : -20.3\n",
            "Q loss: 0.0002811040903907269\n",
            "Episode : 145 , episodic length : 857, return: -20.0, exp_count :130430, averged returnover past 100 episodes : -20.3\n",
            "Episode : 146 , episodic length : 986, return: -20.0, exp_count :131417, averged returnover past 100 episodes : -20.29\n",
            "Episode : 147 , episodic length : 869, return: -20.0, exp_count :132287, averged returnover past 100 episodes : -20.28\n",
            "Episode : 148 , episodic length : 939, return: -21.0, exp_count :133227, averged returnover past 100 episodes : -20.3\n",
            "Q loss: 0.00023060131934471428\n",
            "Episode : 149 , episodic length : 816, return: -21.0, exp_count :134044, averged returnover past 100 episodes : -20.3\n",
            "Episode : 150 , episodic length : 914, return: -20.0, exp_count :134959, averged returnover past 100 episodes : -20.34\n",
            "Episode : 151 , episodic length : 877, return: -21.0, exp_count :135837, averged returnover past 100 episodes : -20.34\n",
            "Episode : 152 , episodic length : 843, return: -21.0, exp_count :136681, averged returnover past 100 episodes : -20.34\n",
            "Episode : 153 , episodic length : 847, return: -21.0, exp_count :137529, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 0.0002530195051804185\n",
            "Episode : 154 , episodic length : 955, return: -21.0, exp_count :138485, averged returnover past 100 episodes : -20.34\n",
            "Episode : 155 , episodic length : 1162, return: -20.0, exp_count :139648, averged returnover past 100 episodes : -20.33\n",
            "Episode : 156 , episodic length : 952, return: -21.0, exp_count :140601, averged returnover past 100 episodes : -20.34\n",
            "Episode : 157 , episodic length : 833, return: -20.0, exp_count :141435, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 8.646229980513453e-05\n",
            "Episode : 158 , episodic length : 927, return: -20.0, exp_count :142363, averged returnover past 100 episodes : -20.36\n",
            "Episode : 159 , episodic length : 1044, return: -20.0, exp_count :143408, averged returnover past 100 episodes : -20.37\n",
            "Episode : 160 , episodic length : 941, return: -20.0, exp_count :144350, averged returnover past 100 episodes : -20.37\n",
            "Episode : 161 , episodic length : 868, return: -21.0, exp_count :145219, averged returnover past 100 episodes : -20.37\n",
            "Q loss: 0.0002459643001202494\n",
            "Episode : 162 , episodic length : 927, return: -21.0, exp_count :146147, averged returnover past 100 episodes : -20.38\n",
            "Episode : 163 , episodic length : 978, return: -19.0, exp_count :147126, averged returnover past 100 episodes : -20.36\n",
            "Episode : 164 , episodic length : 774, return: -21.0, exp_count :147901, averged returnover past 100 episodes : -20.37\n",
            "Episode : 165 , episodic length : 925, return: -21.0, exp_count :148827, averged returnover past 100 episodes : -20.37\n",
            "Episode : 166 , episodic length : 896, return: -21.0, exp_count :149724, averged returnover past 100 episodes : -20.37\n",
            "Q loss: 0.00029220053693279624\n",
            "Episode : 167 , episodic length : 913, return: -21.0, exp_count :150638, averged returnover past 100 episodes : -20.38\n",
            "Episode : 168 , episodic length : 1129, return: -19.0, exp_count :151768, averged returnover past 100 episodes : -20.38\n",
            "Episode : 169 , episodic length : 1005, return: -20.0, exp_count :152774, averged returnover past 100 episodes : -20.37\n",
            "Episode : 170 , episodic length : 907, return: -20.0, exp_count :153682, averged returnover past 100 episodes : -20.36\n",
            "Q loss: 0.00013971791486255825\n",
            "Episode : 171 , episodic length : 1018, return: -20.0, exp_count :154701, averged returnover past 100 episodes : -20.35\n",
            "Episode : 172 , episodic length : 1048, return: -21.0, exp_count :155750, averged returnover past 100 episodes : -20.35\n",
            "Episode : 173 , episodic length : 849, return: -21.0, exp_count :156600, averged returnover past 100 episodes : -20.36\n",
            "Episode : 174 , episodic length : 861, return: -21.0, exp_count :157462, averged returnover past 100 episodes : -20.36\n",
            "Q loss: 0.00015162145427893847\n",
            "Episode : 175 , episodic length : 994, return: -21.0, exp_count :158457, averged returnover past 100 episodes : -20.36\n",
            "Episode : 176 , episodic length : 1248, return: -17.0, exp_count :159706, averged returnover past 100 episodes : -20.34\n",
            "Episode : 177 , episodic length : 1001, return: -21.0, exp_count :160708, averged returnover past 100 episodes : -20.35\n",
            "Episode : 178 , episodic length : 937, return: -20.0, exp_count :161646, averged returnover past 100 episodes : -20.34\n",
            "Q loss: 0.00030335623887367547\n",
            "Episode : 179 , episodic length : 1176, return: -17.0, exp_count :162823, averged returnover past 100 episodes : -20.3\n",
            "Episode : 180 , episodic length : 1032, return: -20.0, exp_count :163856, averged returnover past 100 episodes : -20.3\n",
            "Episode : 181 , episodic length : 1232, return: -20.0, exp_count :165089, averged returnover past 100 episodes : -20.3\n",
            "Q loss: 0.00020563916768878698\n",
            "Episode : 182 , episodic length : 1105, return: -20.0, exp_count :166195, averged returnover past 100 episodes : -20.29\n",
            "Episode : 183 , episodic length : 840, return: -21.0, exp_count :167036, averged returnover past 100 episodes : -20.29\n",
            "Episode : 184 , episodic length : 842, return: -21.0, exp_count :167879, averged returnover past 100 episodes : -20.29\n",
            "Episode : 185 , episodic length : 978, return: -21.0, exp_count :168858, averged returnover past 100 episodes : -20.3\n",
            "Episode : 186 , episodic length : 1049, return: -20.0, exp_count :169908, averged returnover past 100 episodes : -20.29\n",
            "Q loss: 0.0002429328887956217\n",
            "Episode : 187 , episodic length : 1014, return: -20.0, exp_count :170923, averged returnover past 100 episodes : -20.3\n",
            "Episode : 188 , episodic length : 1211, return: -18.0, exp_count :172135, averged returnover past 100 episodes : -20.28\n",
            "Episode : 189 , episodic length : 1083, return: -18.0, exp_count :173219, averged returnover past 100 episodes : -20.27\n",
            "Q loss: 0.00014617097622249275\n",
            "Episode : 190 , episodic length : 946, return: -21.0, exp_count :174166, averged returnover past 100 episodes : -20.28\n",
            "Episode : 191 , episodic length : 1012, return: -19.0, exp_count :175179, averged returnover past 100 episodes : -20.28\n",
            "Episode : 192 , episodic length : 1307, return: -19.0, exp_count :176487, averged returnover past 100 episodes : -20.26\n",
            "Episode : 193 , episodic length : 1086, return: -21.0, exp_count :177574, averged returnover past 100 episodes : -20.27\n",
            "Q loss: 0.00020383579249028116\n",
            "Episode : 194 , episodic length : 941, return: -21.0, exp_count :178516, averged returnover past 100 episodes : -20.28\n",
            "Episode : 195 , episodic length : 1127, return: -19.0, exp_count :179644, averged returnover past 100 episodes : -20.26\n",
            "Episode : 196 , episodic length : 1156, return: -20.0, exp_count :180801, averged returnover past 100 episodes : -20.25\n",
            "Episode : 197 , episodic length : 1120, return: -18.0, exp_count :181922, averged returnover past 100 episodes : -20.23\n",
            "Q loss: 0.00019877910381183028\n",
            "Episode : 198 , episodic length : 920, return: -21.0, exp_count :182843, averged returnover past 100 episodes : -20.26\n",
            "Episode : 199 , episodic length : 1115, return: -21.0, exp_count :183959, averged returnover past 100 episodes : -20.27\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-19. -19. -21. -19. -21. -21. -21. -19. -21. -21. -21. -21. -21. -21.\n",
            " -19.]\n",
            "Episode : 200 , episodic length : 1000, return: -21.0, exp_count :184960, averged returnover past 100 episodes : -20.27\n",
            "Episode : 201 , episodic length : 896, return: -21.0, exp_count :185857, averged returnover past 100 episodes : -20.29\n",
            "Q loss: 0.00013947096886113286\n",
            "Episode : 202 , episodic length : 1222, return: -20.0, exp_count :187080, averged returnover past 100 episodes : -20.28\n",
            "Episode : 203 , episodic length : 961, return: -21.0, exp_count :188042, averged returnover past 100 episodes : -20.3\n",
            "Episode : 204 , episodic length : 1132, return: -19.0, exp_count :189175, averged returnover past 100 episodes : -20.28\n",
            "Q loss: 0.00013549363939091563\n",
            "Episode : 205 , episodic length : 917, return: -20.0, exp_count :190093, averged returnover past 100 episodes : -20.27\n",
            "Episode : 206 , episodic length : 893, return: -20.0, exp_count :190987, averged returnover past 100 episodes : -20.27\n",
            "Episode : 207 , episodic length : 1059, return: -19.0, exp_count :192047, averged returnover past 100 episodes : -20.25\n",
            "Episode : 208 , episodic length : 952, return: -21.0, exp_count :193000, averged returnover past 100 episodes : -20.25\n",
            "Episode : 209 , episodic length : 950, return: -20.0, exp_count :193951, averged returnover past 100 episodes : -20.25\n",
            "Q loss: 0.00011254768469370902\n",
            "Episode : 210 , episodic length : 1160, return: -19.0, exp_count :195112, averged returnover past 100 episodes : -20.24\n",
            "Episode : 211 , episodic length : 880, return: -21.0, exp_count :195993, averged returnover past 100 episodes : -20.24\n",
            "Episode : 212 , episodic length : 873, return: -21.0, exp_count :196867, averged returnover past 100 episodes : -20.24\n",
            "Q loss: 0.00012253300519660115\n",
            "Episode : 213 , episodic length : 1286, return: -17.0, exp_count :198154, averged returnover past 100 episodes : -20.21\n",
            "Episode : 214 , episodic length : 986, return: -21.0, exp_count :199141, averged returnover past 100 episodes : -20.21\n",
            "Episode : 215 , episodic length : 1464, return: -19.0, exp_count :200606, averged returnover past 100 episodes : -20.19\n",
            "Episode : 216 , episodic length : 1352, return: -18.0, exp_count :201959, averged returnover past 100 episodes : -20.18\n",
            "Q loss: 0.00014450684830080718\n",
            "Episode : 217 , episodic length : 1104, return: -19.0, exp_count :203064, averged returnover past 100 episodes : -20.16\n",
            "Episode : 218 , episodic length : 1100, return: -21.0, exp_count :204165, averged returnover past 100 episodes : -20.16\n",
            "Episode : 219 , episodic length : 994, return: -20.0, exp_count :205160, averged returnover past 100 episodes : -20.15\n",
            "Q loss: 0.00012943708861712366\n",
            "Episode : 220 , episodic length : 1081, return: -21.0, exp_count :206242, averged returnover past 100 episodes : -20.16\n",
            "Episode : 221 , episodic length : 899, return: -21.0, exp_count :207142, averged returnover past 100 episodes : -20.16\n",
            "Episode : 222 , episodic length : 1009, return: -19.0, exp_count :208152, averged returnover past 100 episodes : -20.16\n",
            "Episode : 223 , episodic length : 959, return: -20.0, exp_count :209112, averged returnover past 100 episodes : -20.15\n",
            "Episode : 224 , episodic length : 865, return: -21.0, exp_count :209978, averged returnover past 100 episodes : -20.17\n",
            "Q loss: 0.00025996792828664184\n",
            "Episode : 225 , episodic length : 921, return: -20.0, exp_count :210900, averged returnover past 100 episodes : -20.16\n",
            "Episode : 226 , episodic length : 1072, return: -21.0, exp_count :211973, averged returnover past 100 episodes : -20.16\n",
            "Episode : 227 , episodic length : 970, return: -21.0, exp_count :212944, averged returnover past 100 episodes : -20.16\n",
            "Q loss: 9.486544877290726e-05\n",
            "Episode : 228 , episodic length : 1259, return: -18.0, exp_count :214204, averged returnover past 100 episodes : -20.13\n",
            "Episode : 229 , episodic length : 1060, return: -21.0, exp_count :215265, averged returnover past 100 episodes : -20.13\n",
            "Episode : 230 , episodic length : 1271, return: -16.0, exp_count :216537, averged returnover past 100 episodes : -20.09\n",
            "Episode : 231 , episodic length : 974, return: -21.0, exp_count :217512, averged returnover past 100 episodes : -20.1\n",
            "Q loss: 9.219189814757556e-05\n",
            "Episode : 232 , episodic length : 1169, return: -17.0, exp_count :218682, averged returnover past 100 episodes : -20.08\n",
            "Episode : 233 , episodic length : 927, return: -20.0, exp_count :219610, averged returnover past 100 episodes : -20.08\n",
            "Episode : 234 , episodic length : 1097, return: -19.0, exp_count :220708, averged returnover past 100 episodes : -20.06\n",
            "Episode : 235 , episodic length : 1049, return: -19.0, exp_count :221758, averged returnover past 100 episodes : -20.06\n",
            "Q loss: 0.00011919432290596887\n",
            "Episode : 236 , episodic length : 976, return: -20.0, exp_count :222735, averged returnover past 100 episodes : -20.06\n",
            "Episode : 237 , episodic length : 946, return: -21.0, exp_count :223682, averged returnover past 100 episodes : -20.06\n",
            "Episode : 238 , episodic length : 957, return: -21.0, exp_count :224640, averged returnover past 100 episodes : -20.06\n",
            "Episode : 239 , episodic length : 1331, return: -17.0, exp_count :225972, averged returnover past 100 episodes : -20.02\n",
            "Q loss: 0.00011902066762559116\n",
            "Episode : 240 , episodic length : 911, return: -19.0, exp_count :226884, averged returnover past 100 episodes : -20.0\n",
            "Episode : 241 , episodic length : 946, return: -20.0, exp_count :227831, averged returnover past 100 episodes : -20.0\n",
            "Episode : 242 , episodic length : 942, return: -20.0, exp_count :228774, averged returnover past 100 episodes : -19.99\n",
            "Episode : 243 , episodic length : 1046, return: -20.0, exp_count :229821, averged returnover past 100 episodes : -20.0\n",
            "Q loss: 6.385343294823542e-05\n",
            "Episode : 244 , episodic length : 1131, return: -19.0, exp_count :230953, averged returnover past 100 episodes : -19.99\n",
            "Episode : 245 , episodic length : 1155, return: -20.0, exp_count :232109, averged returnover past 100 episodes : -19.99\n",
            "Episode : 246 , episodic length : 1280, return: -20.0, exp_count :233390, averged returnover past 100 episodes : -19.99\n",
            "Q loss: 0.00010330616350984201\n",
            "Episode : 247 , episodic length : 891, return: -21.0, exp_count :234282, averged returnover past 100 episodes : -20.0\n",
            "Episode : 248 , episodic length : 1313, return: -20.0, exp_count :235596, averged returnover past 100 episodes : -19.99\n",
            "Episode : 249 , episodic length : 938, return: -20.0, exp_count :236535, averged returnover past 100 episodes : -19.98\n",
            "Episode : 250 , episodic length : 1324, return: -18.0, exp_count :237860, averged returnover past 100 episodes : -19.96\n",
            "Q loss: 9.390679042553529e-05\n",
            "Episode : 251 , episodic length : 1045, return: -20.0, exp_count :238906, averged returnover past 100 episodes : -19.95\n",
            "Episode : 252 , episodic length : 913, return: -20.0, exp_count :239820, averged returnover past 100 episodes : -19.94\n",
            "Episode : 253 , episodic length : 1260, return: -20.0, exp_count :241081, averged returnover past 100 episodes : -19.93\n",
            "Q loss: 6.29201895208098e-05\n",
            "Episode : 254 , episodic length : 1074, return: -19.0, exp_count :242156, averged returnover past 100 episodes : -19.91\n",
            "Episode : 255 , episodic length : 906, return: -20.0, exp_count :243063, averged returnover past 100 episodes : -19.91\n",
            "Episode : 256 , episodic length : 978, return: -20.0, exp_count :244042, averged returnover past 100 episodes : -19.9\n",
            "Episode : 257 , episodic length : 1121, return: -20.0, exp_count :245164, averged returnover past 100 episodes : -19.9\n",
            "Q loss: 6.430411303881556e-05\n",
            "Episode : 258 , episodic length : 1127, return: -20.0, exp_count :246292, averged returnover past 100 episodes : -19.9\n",
            "Episode : 259 , episodic length : 884, return: -20.0, exp_count :247177, averged returnover past 100 episodes : -19.9\n",
            "Episode : 260 , episodic length : 1163, return: -19.0, exp_count :248341, averged returnover past 100 episodes : -19.89\n",
            "Episode : 261 , episodic length : 1158, return: -17.0, exp_count :249500, averged returnover past 100 episodes : -19.85\n",
            "Q loss: 5.446681461762637e-05\n",
            "Episode : 262 , episodic length : 983, return: -19.0, exp_count :250484, averged returnover past 100 episodes : -19.83\n",
            "Episode : 263 , episodic length : 1397, return: -18.0, exp_count :251882, averged returnover past 100 episodes : -19.82\n",
            "Episode : 264 , episodic length : 927, return: -21.0, exp_count :252810, averged returnover past 100 episodes : -19.82\n",
            "Q loss: 5.45349539606832e-05\n",
            "Episode : 265 , episodic length : 1303, return: -17.0, exp_count :254114, averged returnover past 100 episodes : -19.78\n",
            "Episode : 266 , episodic length : 989, return: -21.0, exp_count :255104, averged returnover past 100 episodes : -19.78\n",
            "Episode : 267 , episodic length : 1058, return: -19.0, exp_count :256163, averged returnover past 100 episodes : -19.76\n",
            "Episode : 268 , episodic length : 1247, return: -18.0, exp_count :257411, averged returnover past 100 episodes : -19.75\n",
            "Q loss: 6.514048436656594e-05\n",
            "Episode : 269 , episodic length : 994, return: -21.0, exp_count :258406, averged returnover past 100 episodes : -19.76\n",
            "Episode : 270 , episodic length : 1201, return: -19.0, exp_count :259608, averged returnover past 100 episodes : -19.75\n",
            "Episode : 271 , episodic length : 1223, return: -20.0, exp_count :260832, averged returnover past 100 episodes : -19.75\n",
            "Q loss: 0.00010333213140256703\n",
            "Episode : 272 , episodic length : 1190, return: -20.0, exp_count :262023, averged returnover past 100 episodes : -19.74\n",
            "Episode : 273 , episodic length : 1168, return: -20.0, exp_count :263192, averged returnover past 100 episodes : -19.73\n",
            "Episode : 274 , episodic length : 1044, return: -19.0, exp_count :264237, averged returnover past 100 episodes : -19.71\n",
            "Episode : 275 , episodic length : 984, return: -20.0, exp_count :265222, averged returnover past 100 episodes : -19.7\n",
            "Q loss: 6.453248352045193e-05\n",
            "Episode : 276 , episodic length : 1202, return: -18.0, exp_count :266425, averged returnover past 100 episodes : -19.71\n",
            "Episode : 277 , episodic length : 1444, return: -18.0, exp_count :267870, averged returnover past 100 episodes : -19.68\n",
            "Episode : 278 , episodic length : 1418, return: -18.0, exp_count :269289, averged returnover past 100 episodes : -19.66\n",
            "Q loss: 8.489841275149956e-05\n",
            "Episode : 279 , episodic length : 1209, return: -20.0, exp_count :270499, averged returnover past 100 episodes : -19.69\n",
            "Episode : 280 , episodic length : 1592, return: -17.0, exp_count :272092, averged returnover past 100 episodes : -19.66\n",
            "Episode : 281 , episodic length : 1230, return: -20.0, exp_count :273323, averged returnover past 100 episodes : -19.66\n",
            "Q loss: 7.462871872121468e-05\n",
            "Episode : 282 , episodic length : 1420, return: -19.0, exp_count :274744, averged returnover past 100 episodes : -19.65\n",
            "Episode : 283 , episodic length : 1327, return: -19.0, exp_count :276072, averged returnover past 100 episodes : -19.63\n",
            "Episode : 284 , episodic length : 1163, return: -19.0, exp_count :277236, averged returnover past 100 episodes : -19.61\n",
            "Q loss: 8.660370076540858e-05\n",
            "Episode : 285 , episodic length : 1134, return: -21.0, exp_count :278371, averged returnover past 100 episodes : -19.61\n",
            "Episode : 286 , episodic length : 1119, return: -20.0, exp_count :279491, averged returnover past 100 episodes : -19.61\n",
            "Episode : 287 , episodic length : 1014, return: -21.0, exp_count :280506, averged returnover past 100 episodes : -19.62\n",
            "Episode : 288 , episodic length : 1238, return: -20.0, exp_count :281745, averged returnover past 100 episodes : -19.64\n",
            "Q loss: 0.00012789334869012237\n",
            "Episode : 289 , episodic length : 1037, return: -19.0, exp_count :282783, averged returnover past 100 episodes : -19.65\n",
            "Episode : 290 , episodic length : 1363, return: -17.0, exp_count :284147, averged returnover past 100 episodes : -19.61\n",
            "Episode : 291 , episodic length : 1368, return: -17.0, exp_count :285516, averged returnover past 100 episodes : -19.59\n",
            "Q loss: 4.450623600860126e-05\n",
            "Episode : 292 , episodic length : 1599, return: -16.0, exp_count :287116, averged returnover past 100 episodes : -19.56\n",
            "Episode : 293 , episodic length : 1136, return: -19.0, exp_count :288253, averged returnover past 100 episodes : -19.54\n",
            "Episode : 294 , episodic length : 1245, return: -21.0, exp_count :289499, averged returnover past 100 episodes : -19.54\n",
            "Q loss: 6.873659003758803e-05\n",
            "Episode : 295 , episodic length : 1347, return: -19.0, exp_count :290847, averged returnover past 100 episodes : -19.54\n",
            "Episode : 296 , episodic length : 1412, return: -18.0, exp_count :292260, averged returnover past 100 episodes : -19.52\n",
            "Episode : 297 , episodic length : 1283, return: -18.0, exp_count :293544, averged returnover past 100 episodes : -19.52\n",
            "Q loss: 6.217929330887273e-05\n",
            "Episode : 298 , episodic length : 1538, return: -17.0, exp_count :295083, averged returnover past 100 episodes : -19.48\n",
            "Episode : 299 , episodic length : 1194, return: -19.0, exp_count :296278, averged returnover past 100 episodes : -19.46\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-21. -20. -21. -20. -21. -21. -21. -21. -21. -21. -21. -20. -21. -21.\n",
            " -21.]\n",
            "Episode : 300 , episodic length : 1458, return: -16.0, exp_count :297737, averged returnover past 100 episodes : -19.41\n",
            "Q loss: 4.070835348102264e-05\n",
            "Episode : 301 , episodic length : 1450, return: -16.0, exp_count :299188, averged returnover past 100 episodes : -19.36\n",
            "Episode : 302 , episodic length : 1495, return: -16.0, exp_count :300684, averged returnover past 100 episodes : -19.32\n",
            "Q loss: 7.507868576794863e-05\n",
            "Episode : 303 , episodic length : 1677, return: -15.0, exp_count :302362, averged returnover past 100 episodes : -19.26\n",
            "Episode : 304 , episodic length : 1277, return: -19.0, exp_count :303640, averged returnover past 100 episodes : -19.26\n",
            "Episode : 305 , episodic length : 1450, return: -19.0, exp_count :305091, averged returnover past 100 episodes : -19.25\n",
            "Q loss: 0.0001417821185896173\n",
            "Episode : 306 , episodic length : 1571, return: -14.0, exp_count :306663, averged returnover past 100 episodes : -19.19\n",
            "Episode : 307 , episodic length : 1314, return: -17.0, exp_count :307978, averged returnover past 100 episodes : -19.17\n",
            "Episode : 308 , episodic length : 1254, return: -19.0, exp_count :309233, averged returnover past 100 episodes : -19.15\n",
            "Q loss: 0.0001383155322400853\n",
            "Episode : 309 , episodic length : 1508, return: -17.0, exp_count :310742, averged returnover past 100 episodes : -19.12\n",
            "Episode : 310 , episodic length : 1257, return: -19.0, exp_count :312000, averged returnover past 100 episodes : -19.12\n",
            "Episode : 311 , episodic length : 1322, return: -20.0, exp_count :313323, averged returnover past 100 episodes : -19.11\n",
            "Q loss: 3.1456660508411005e-05\n",
            "Episode : 312 , episodic length : 2048, return: -14.0, exp_count :315372, averged returnover past 100 episodes : -19.04\n",
            "Episode : 313 , episodic length : 1055, return: -20.0, exp_count :316428, averged returnover past 100 episodes : -19.07\n",
            "Episode : 314 , episodic length : 1486, return: -15.0, exp_count :317915, averged returnover past 100 episodes : -19.01\n",
            "Q loss: 2.4041655706241727e-05\n",
            "Episode : 315 , episodic length : 1290, return: -21.0, exp_count :319206, averged returnover past 100 episodes : -19.03\n",
            "Episode : 316 , episodic length : 1252, return: -18.0, exp_count :320459, averged returnover past 100 episodes : -19.03\n",
            "Q loss: 4.4576947402674705e-05\n",
            "Episode : 317 , episodic length : 1594, return: -17.0, exp_count :322054, averged returnover past 100 episodes : -19.01\n",
            "Episode : 318 , episodic length : 1753, return: -18.0, exp_count :323808, averged returnover past 100 episodes : -18.98\n",
            "Episode : 319 , episodic length : 1764, return: -17.0, exp_count :325573, averged returnover past 100 episodes : -18.95\n",
            "Q loss: 6.30800350336358e-05\n",
            "Episode : 320 , episodic length : 1358, return: -16.0, exp_count :326932, averged returnover past 100 episodes : -18.9\n",
            "Episode : 321 , episodic length : 1413, return: -18.0, exp_count :328346, averged returnover past 100 episodes : -18.87\n",
            "Q loss: 4.115232877666131e-05\n",
            "Episode : 322 , episodic length : 2049, return: -16.0, exp_count :330396, averged returnover past 100 episodes : -18.84\n",
            "Episode : 323 , episodic length : 1203, return: -19.0, exp_count :331600, averged returnover past 100 episodes : -18.83\n",
            "Episode : 324 , episodic length : 1665, return: -15.0, exp_count :333266, averged returnover past 100 episodes : -18.77\n",
            "Q loss: 1.1545085726538673e-05\n",
            "Episode : 325 , episodic length : 1365, return: -17.0, exp_count :334632, averged returnover past 100 episodes : -18.74\n",
            "Episode : 326 , episodic length : 1179, return: -19.0, exp_count :335812, averged returnover past 100 episodes : -18.72\n",
            "Episode : 327 , episodic length : 1439, return: -18.0, exp_count :337252, averged returnover past 100 episodes : -18.69\n",
            "Q loss: 1.9790486476267688e-05\n",
            "Episode : 328 , episodic length : 1217, return: -19.0, exp_count :338470, averged returnover past 100 episodes : -18.7\n",
            "Episode : 329 , episodic length : 1099, return: -21.0, exp_count :339570, averged returnover past 100 episodes : -18.7\n",
            "Episode : 330 , episodic length : 1220, return: -18.0, exp_count :340791, averged returnover past 100 episodes : -18.72\n",
            "Q loss: 2.2206848370842636e-05\n",
            "Episode : 331 , episodic length : 1496, return: -18.0, exp_count :342288, averged returnover past 100 episodes : -18.69\n",
            "Episode : 332 , episodic length : 1534, return: -18.0, exp_count :343823, averged returnover past 100 episodes : -18.7\n",
            "Episode : 333 , episodic length : 1835, return: -13.0, exp_count :345659, averged returnover past 100 episodes : -18.63\n",
            "Q loss: 3.0620059987995774e-05\n",
            "Episode : 334 , episodic length : 1583, return: -20.0, exp_count :347243, averged returnover past 100 episodes : -18.64\n",
            "Episode : 335 , episodic length : 2247, return: -12.0, exp_count :349491, averged returnover past 100 episodes : -18.57\n",
            "Q loss: 2.6367331884102896e-05\n",
            "Episode : 336 , episodic length : 1880, return: -15.0, exp_count :351372, averged returnover past 100 episodes : -18.52\n",
            "Episode : 337 , episodic length : 1546, return: -16.0, exp_count :352919, averged returnover past 100 episodes : -18.47\n",
            "Q loss: 2.6613257432472892e-05\n",
            "Episode : 338 , episodic length : 1599, return: -16.0, exp_count :354519, averged returnover past 100 episodes : -18.42\n",
            "Episode : 339 , episodic length : 1483, return: -17.0, exp_count :356003, averged returnover past 100 episodes : -18.42\n",
            "Episode : 340 , episodic length : 1532, return: -19.0, exp_count :357536, averged returnover past 100 episodes : -18.42\n",
            "Q loss: 2.135344038833864e-05\n",
            "Episode : 341 , episodic length : 1441, return: -17.0, exp_count :358978, averged returnover past 100 episodes : -18.39\n",
            "Episode : 342 , episodic length : 1526, return: -16.0, exp_count :360505, averged returnover past 100 episodes : -18.35\n",
            "Episode : 343 , episodic length : 1312, return: -17.0, exp_count :361818, averged returnover past 100 episodes : -18.32\n",
            "Q loss: 2.6167292162426747e-05\n",
            "Episode : 344 , episodic length : 1641, return: -18.0, exp_count :363460, averged returnover past 100 episodes : -18.31\n",
            "Episode : 345 , episodic length : 1652, return: -15.0, exp_count :365113, averged returnover past 100 episodes : -18.26\n",
            "Q loss: 2.931496055680327e-05\n",
            "Episode : 346 , episodic length : 1756, return: -16.0, exp_count :366870, averged returnover past 100 episodes : -18.22\n",
            "Episode : 347 , episodic length : 1242, return: -20.0, exp_count :368113, averged returnover past 100 episodes : -18.21\n",
            "Episode : 348 , episodic length : 1344, return: -18.0, exp_count :369458, averged returnover past 100 episodes : -18.19\n",
            "Q loss: 3.239156285417266e-05\n",
            "Episode : 349 , episodic length : 1510, return: -16.0, exp_count :370969, averged returnover past 100 episodes : -18.15\n",
            "Episode : 350 , episodic length : 1518, return: -15.0, exp_count :372488, averged returnover past 100 episodes : -18.12\n",
            "Q loss: 1.2843515833083075e-05\n",
            "Episode : 351 , episodic length : 1510, return: -17.0, exp_count :373999, averged returnover past 100 episodes : -18.09\n",
            "Episode : 352 , episodic length : 1013, return: -21.0, exp_count :375013, averged returnover past 100 episodes : -18.1\n",
            "Episode : 353 , episodic length : 1238, return: -20.0, exp_count :376252, averged returnover past 100 episodes : -18.1\n",
            "Episode : 354 , episodic length : 1702, return: -15.0, exp_count :377955, averged returnover past 100 episodes : -18.06\n",
            "Q loss: 2.776666406134609e-05\n",
            "Episode : 355 , episodic length : 1866, return: -14.0, exp_count :379822, averged returnover past 100 episodes : -18.0\n",
            "Episode : 356 , episodic length : 1513, return: -18.0, exp_count :381336, averged returnover past 100 episodes : -17.98\n",
            "Q loss: 2.8599690267583355e-05\n",
            "Episode : 357 , episodic length : 1841, return: -13.0, exp_count :383178, averged returnover past 100 episodes : -17.91\n",
            "Episode : 358 , episodic length : 1429, return: -20.0, exp_count :384608, averged returnover past 100 episodes : -17.91\n",
            "Episode : 359 , episodic length : 1379, return: -17.0, exp_count :385988, averged returnover past 100 episodes : -17.88\n",
            "Q loss: 2.8364349418552592e-05\n",
            "Episode : 360 , episodic length : 1927, return: -15.0, exp_count :387916, averged returnover past 100 episodes : -17.84\n",
            "Episode : 361 , episodic length : 1474, return: -16.0, exp_count :389391, averged returnover past 100 episodes : -17.83\n",
            "Q loss: 1.4639828805229627e-05\n",
            "Episode : 362 , episodic length : 1613, return: -17.0, exp_count :391005, averged returnover past 100 episodes : -17.81\n",
            "Episode : 363 , episodic length : 1656, return: -18.0, exp_count :392662, averged returnover past 100 episodes : -17.81\n",
            "Q loss: 2.917867823271081e-05\n",
            "Episode : 364 , episodic length : 1821, return: -15.0, exp_count :394484, averged returnover past 100 episodes : -17.75\n",
            "Episode : 365 , episodic length : 1431, return: -17.0, exp_count :395916, averged returnover past 100 episodes : -17.75\n",
            "Episode : 366 , episodic length : 1627, return: -17.0, exp_count :397544, averged returnover past 100 episodes : -17.71\n",
            "Q loss: 3.1513278372585773e-05\n",
            "Episode : 367 , episodic length : 1694, return: -14.0, exp_count :399239, averged returnover past 100 episodes : -17.66\n",
            "Episode : 368 , episodic length : 1633, return: -15.0, exp_count :400873, averged returnover past 100 episodes : -17.63\n",
            "Q loss: 2.2369102225638926e-05\n",
            "Episode : 369 , episodic length : 1537, return: -17.0, exp_count :402411, averged returnover past 100 episodes : -17.59\n",
            "Episode : 370 , episodic length : 1912, return: -17.0, exp_count :404324, averged returnover past 100 episodes : -17.57\n",
            "Episode : 371 , episodic length : 1662, return: -20.0, exp_count :405987, averged returnover past 100 episodes : -17.57\n",
            "Q loss: 1.5170251572271809e-05\n",
            "Episode : 372 , episodic length : 2226, return: -11.0, exp_count :408214, averged returnover past 100 episodes : -17.48\n",
            "Q loss: 1.0900664165092167e-05\n",
            "Episode : 373 , episodic length : 2059, return: -15.0, exp_count :410274, averged returnover past 100 episodes : -17.43\n",
            "Episode : 374 , episodic length : 1911, return: -12.0, exp_count :412186, averged returnover past 100 episodes : -17.36\n",
            "Episode : 375 , episodic length : 1758, return: -15.0, exp_count :413945, averged returnover past 100 episodes : -17.31\n",
            "Q loss: 1.0432968338136561e-05\n",
            "Episode : 376 , episodic length : 1438, return: -17.0, exp_count :415384, averged returnover past 100 episodes : -17.3\n",
            "Episode : 377 , episodic length : 1594, return: -18.0, exp_count :416979, averged returnover past 100 episodes : -17.3\n",
            "Q loss: 3.959707100875676e-05\n",
            "Episode : 378 , episodic length : 1941, return: -13.0, exp_count :418921, averged returnover past 100 episodes : -17.25\n",
            "Episode : 379 , episodic length : 1629, return: -19.0, exp_count :420551, averged returnover past 100 episodes : -17.24\n",
            "Q loss: 5.45376751688309e-05\n",
            "Episode : 380 , episodic length : 1717, return: -16.0, exp_count :422269, averged returnover past 100 episodes : -17.23\n",
            "Episode : 381 , episodic length : 1479, return: -17.0, exp_count :423749, averged returnover past 100 episodes : -17.2\n",
            "Episode : 382 , episodic length : 1418, return: -16.0, exp_count :425168, averged returnover past 100 episodes : -17.17\n",
            "Q loss: 1.2564998542075045e-05\n",
            "Episode : 383 , episodic length : 2118, return: -16.0, exp_count :427287, averged returnover past 100 episodes : -17.14\n",
            "Episode : 384 , episodic length : 1620, return: -17.0, exp_count :428908, averged returnover past 100 episodes : -17.12\n",
            "Q loss: 9.165893970930483e-06\n",
            "Episode : 385 , episodic length : 1786, return: -15.0, exp_count :430695, averged returnover past 100 episodes : -17.06\n",
            "Episode : 386 , episodic length : 1661, return: -16.0, exp_count :432357, averged returnover past 100 episodes : -17.02\n",
            "Episode : 387 , episodic length : 1627, return: -16.0, exp_count :433985, averged returnover past 100 episodes : -16.97\n",
            "Q loss: 9.416109605808742e-06\n",
            "Episode : 388 , episodic length : 2183, return: -12.0, exp_count :436169, averged returnover past 100 episodes : -16.89\n",
            "Episode : 389 , episodic length : 1712, return: -18.0, exp_count :437882, averged returnover past 100 episodes : -16.88\n",
            "Q loss: 2.2763591914554127e-05\n",
            "Episode : 390 , episodic length : 1442, return: -18.0, exp_count :439325, averged returnover past 100 episodes : -16.89\n",
            "Episode : 391 , episodic length : 1712, return: -14.0, exp_count :441038, averged returnover past 100 episodes : -16.86\n",
            "Q loss: 5.878991487406893e-06\n",
            "Episode : 392 , episodic length : 1599, return: -15.0, exp_count :442638, averged returnover past 100 episodes : -16.85\n",
            "Episode : 393 , episodic length : 1605, return: -14.0, exp_count :444244, averged returnover past 100 episodes : -16.8\n",
            "Episode : 394 , episodic length : 1688, return: -17.0, exp_count :445933, averged returnover past 100 episodes : -16.76\n",
            "Q loss: 8.798390808806289e-06\n",
            "Episode : 395 , episodic length : 1727, return: -17.0, exp_count :447661, averged returnover past 100 episodes : -16.74\n",
            "Episode : 396 , episodic length : 1559, return: -18.0, exp_count :449221, averged returnover past 100 episodes : -16.74\n",
            "Q loss: 5.9407020671642385e-06\n",
            "Episode : 397 , episodic length : 1555, return: -17.0, exp_count :450777, averged returnover past 100 episodes : -16.73\n",
            "Episode : 398 , episodic length : 1840, return: -14.0, exp_count :452618, averged returnover past 100 episodes : -16.7\n",
            "Q loss: 3.855275735986652e-06\n",
            "Episode : 399 , episodic length : 2103, return: -12.0, exp_count :454722, averged returnover past 100 episodes : -16.63\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-13. -19. -13. -21. -21. -13. -13. -21. -19. -13. -21. -14. -21. -21.\n",
            " -21.]\n",
            "Episode : 400 , episodic length : 1681, return: -14.0, exp_count :456404, averged returnover past 100 episodes : -16.61\n",
            "Episode : 401 , episodic length : 1562, return: -14.0, exp_count :457967, averged returnover past 100 episodes : -16.59\n",
            "Q loss: 1.1951789929298684e-05\n",
            "Episode : 402 , episodic length : 1660, return: -17.0, exp_count :459628, averged returnover past 100 episodes : -16.6\n",
            "Episode : 403 , episodic length : 1803, return: -13.0, exp_count :461432, averged returnover past 100 episodes : -16.58\n",
            "Q loss: 3.138094689347781e-05\n",
            "Episode : 404 , episodic length : 1862, return: -15.0, exp_count :463295, averged returnover past 100 episodes : -16.54\n",
            "Episode : 405 , episodic length : 1716, return: -14.0, exp_count :465012, averged returnover past 100 episodes : -16.49\n",
            "Q loss: 7.908633051556535e-06\n",
            "Episode : 406 , episodic length : 1781, return: -15.0, exp_count :466794, averged returnover past 100 episodes : -16.5\n",
            "Episode : 407 , episodic length : 1475, return: -17.0, exp_count :468270, averged returnover past 100 episodes : -16.5\n",
            "Q loss: 1.2215595234010834e-05\n",
            "Episode : 408 , episodic length : 1855, return: -16.0, exp_count :470126, averged returnover past 100 episodes : -16.47\n",
            "Episode : 409 , episodic length : 2198, return: -11.0, exp_count :472325, averged returnover past 100 episodes : -16.41\n",
            "Q loss: 9.246624358638655e-06\n",
            "Episode : 410 , episodic length : 1826, return: -16.0, exp_count :474152, averged returnover past 100 episodes : -16.38\n",
            "Episode : 411 , episodic length : 1835, return: -16.0, exp_count :475988, averged returnover past 100 episodes : -16.34\n",
            "Q loss: 7.552095667051617e-06\n",
            "Episode : 412 , episodic length : 2339, return: -14.0, exp_count :478328, averged returnover past 100 episodes : -16.34\n",
            "Episode : 413 , episodic length : 1459, return: -18.0, exp_count :479788, averged returnover past 100 episodes : -16.32\n",
            "Episode : 414 , episodic length : 1845, return: -14.0, exp_count :481634, averged returnover past 100 episodes : -16.31\n",
            "Q loss: 5.410172434494598e-06\n",
            "Episode : 415 , episodic length : 2458, return: -7.0, exp_count :484093, averged returnover past 100 episodes : -16.17\n",
            "Episode : 416 , episodic length : 1798, return: -15.0, exp_count :485892, averged returnover past 100 episodes : -16.14\n",
            "Q loss: 9.989305908675306e-06\n",
            "Episode : 417 , episodic length : 1637, return: -15.0, exp_count :487530, averged returnover past 100 episodes : -16.12\n",
            "Episode : 418 , episodic length : 1655, return: -14.0, exp_count :489186, averged returnover past 100 episodes : -16.08\n",
            "Q loss: 1.1060076758440118e-05\n",
            "Episode : 419 , episodic length : 2107, return: -11.0, exp_count :491294, averged returnover past 100 episodes : -16.02\n",
            "Episode : 420 , episodic length : 1909, return: -13.0, exp_count :493204, averged returnover past 100 episodes : -15.99\n",
            "Q loss: 5.90074614592595e-06\n",
            "Episode : 421 , episodic length : 2106, return: -11.0, exp_count :495311, averged returnover past 100 episodes : -15.92\n",
            "Episode : 422 , episodic length : 1739, return: -15.0, exp_count :497051, averged returnover past 100 episodes : -15.91\n",
            "Q loss: 5.554043582378654e-06\n",
            "Episode : 423 , episodic length : 1904, return: -13.0, exp_count :498956, averged returnover past 100 episodes : -15.85\n",
            "Episode : 424 , episodic length : 1516, return: -15.0, exp_count :500473, averged returnover past 100 episodes : -15.85\n",
            "Q loss: 6.9546567829092965e-06\n",
            "Episode : 425 , episodic length : 1632, return: -16.0, exp_count :502106, averged returnover past 100 episodes : -15.84\n",
            "Episode : 426 , episodic length : 1503, return: -15.0, exp_count :503610, averged returnover past 100 episodes : -15.8\n",
            "Episode : 427 , episodic length : 2096, return: -13.0, exp_count :505707, averged returnover past 100 episodes : -15.75\n",
            "Q loss: 9.197798135573976e-06\n",
            "Episode : 428 , episodic length : 1826, return: -14.0, exp_count :507534, averged returnover past 100 episodes : -15.7\n",
            "Episode : 429 , episodic length : 1530, return: -19.0, exp_count :509065, averged returnover past 100 episodes : -15.68\n",
            "Q loss: 7.425133844662923e-06\n",
            "Episode : 430 , episodic length : 1795, return: -15.0, exp_count :510861, averged returnover past 100 episodes : -15.65\n",
            "Episode : 431 , episodic length : 1686, return: -18.0, exp_count :512548, averged returnover past 100 episodes : -15.65\n",
            "Q loss: 9.790151125343982e-06\n",
            "Episode : 432 , episodic length : 1898, return: -14.0, exp_count :514447, averged returnover past 100 episodes : -15.61\n",
            "Episode : 433 , episodic length : 2170, return: -11.0, exp_count :516618, averged returnover past 100 episodes : -15.59\n",
            "Q loss: 7.957951311254874e-06\n",
            "Episode : 434 , episodic length : 1804, return: -15.0, exp_count :518423, averged returnover past 100 episodes : -15.54\n",
            "Episode : 435 , episodic length : 1481, return: -16.0, exp_count :519905, averged returnover past 100 episodes : -15.58\n",
            "Episode : 436 , episodic length : 1888, return: -14.0, exp_count :521794, averged returnover past 100 episodes : -15.57\n",
            "Q loss: 1.1674351299006958e-05\n",
            "Episode : 437 , episodic length : 1406, return: -20.0, exp_count :523201, averged returnover past 100 episodes : -15.61\n",
            "Episode : 438 , episodic length : 1502, return: -21.0, exp_count :524704, averged returnover past 100 episodes : -15.66\n",
            "Q loss: 4.103632818441838e-06\n",
            "Episode : 439 , episodic length : 2143, return: -12.0, exp_count :526848, averged returnover past 100 episodes : -15.61\n",
            "Episode : 440 , episodic length : 1925, return: -14.0, exp_count :528774, averged returnover past 100 episodes : -15.56\n",
            "Q loss: 7.098546120687388e-06\n",
            "Episode : 441 , episodic length : 1860, return: -15.0, exp_count :530635, averged returnover past 100 episodes : -15.54\n",
            "Episode : 442 , episodic length : 2145, return: -11.0, exp_count :532781, averged returnover past 100 episodes : -15.49\n",
            "Q loss: 7.1867766564537305e-06\n",
            "Episode : 443 , episodic length : 1998, return: -12.0, exp_count :534780, averged returnover past 100 episodes : -15.44\n",
            "Episode : 444 , episodic length : 2324, return: -11.0, exp_count :537105, averged returnover past 100 episodes : -15.37\n",
            "Q loss: 9.058732757694088e-06\n",
            "Episode : 445 , episodic length : 2014, return: -11.0, exp_count :539120, averged returnover past 100 episodes : -15.33\n",
            "Episode : 446 , episodic length : 1972, return: -13.0, exp_count :541093, averged returnover past 100 episodes : -15.3\n",
            "Q loss: 3.858852323901374e-06\n",
            "Episode : 447 , episodic length : 1519, return: -17.0, exp_count :542613, averged returnover past 100 episodes : -15.27\n",
            "Episode : 448 , episodic length : 1853, return: -16.0, exp_count :544467, averged returnover past 100 episodes : -15.25\n",
            "Q loss: 5.702117050532252e-06\n",
            "Episode : 449 , episodic length : 1710, return: -17.0, exp_count :546178, averged returnover past 100 episodes : -15.26\n",
            "Episode : 450 , episodic length : 1699, return: -16.0, exp_count :547878, averged returnover past 100 episodes : -15.27\n",
            "Q loss: 7.824609383533243e-06\n",
            "Episode : 451 , episodic length : 2452, return: -12.0, exp_count :550331, averged returnover past 100 episodes : -15.22\n",
            "Episode : 452 , episodic length : 1683, return: -17.0, exp_count :552015, averged returnover past 100 episodes : -15.18\n",
            "Q loss: 6.524323907797225e-06\n",
            "Episode : 453 , episodic length : 2022, return: -17.0, exp_count :554038, averged returnover past 100 episodes : -15.15\n",
            "Episode : 454 , episodic length : 1897, return: -14.0, exp_count :555936, averged returnover past 100 episodes : -15.14\n",
            "Q loss: 7.517237008869415e-06\n",
            "Episode : 455 , episodic length : 2209, return: -12.0, exp_count :558146, averged returnover past 100 episodes : -15.12\n",
            "Episode : 456 , episodic length : 1574, return: -15.0, exp_count :559721, averged returnover past 100 episodes : -15.09\n",
            "Episode : 457 , episodic length : 2234, return: -15.0, exp_count :561956, averged returnover past 100 episodes : -15.11\n",
            "Q loss: 6.3658344515715726e-06\n",
            "Episode : 458 , episodic length : 1802, return: -17.0, exp_count :563759, averged returnover past 100 episodes : -15.08\n",
            "Episode : 459 , episodic length : 2119, return: -11.0, exp_count :565879, averged returnover past 100 episodes : -15.02\n",
            "Q loss: 7.051697139104363e-06\n",
            "Episode : 460 , episodic length : 1784, return: -17.0, exp_count :567664, averged returnover past 100 episodes : -15.04\n",
            "Episode : 461 , episodic length : 2160, return: -14.0, exp_count :569825, averged returnover past 100 episodes : -15.02\n",
            "Q loss: 8.884631824912503e-06\n",
            "Episode : 462 , episodic length : 2204, return: -11.0, exp_count :572030, averged returnover past 100 episodes : -14.96\n",
            "Episode : 463 , episodic length : 1691, return: -17.0, exp_count :573722, averged returnover past 100 episodes : -14.95\n",
            "Q loss: 6.672471954516368e-06\n",
            "Episode : 464 , episodic length : 1537, return: -18.0, exp_count :575260, averged returnover past 100 episodes : -14.98\n",
            "Episode : 465 , episodic length : 1684, return: -18.0, exp_count :576945, averged returnover past 100 episodes : -14.99\n",
            "Q loss: 7.265011845447589e-06\n",
            "Episode : 466 , episodic length : 1694, return: -14.0, exp_count :578640, averged returnover past 100 episodes : -14.96\n",
            "Episode : 467 , episodic length : 1495, return: -18.0, exp_count :580136, averged returnover past 100 episodes : -15.0\n",
            "Q loss: 6.580620265594916e-06\n",
            "Episode : 468 , episodic length : 2342, return: -13.0, exp_count :582479, averged returnover past 100 episodes : -14.98\n",
            "Episode : 469 , episodic length : 2434, return: -8.0, exp_count :584914, averged returnover past 100 episodes : -14.89\n",
            "Q loss: 7.70219321566401e-06\n",
            "Episode : 470 , episodic length : 3008, return: -6.0, exp_count :587923, averged returnover past 100 episodes : -14.78\n",
            "Q loss: 6.114117240940686e-06\n",
            "Episode : 471 , episodic length : 2098, return: -10.0, exp_count :590022, averged returnover past 100 episodes : -14.68\n",
            "Episode : 472 , episodic length : 1885, return: -17.0, exp_count :591908, averged returnover past 100 episodes : -14.74\n",
            "Episode : 473 , episodic length : 2026, return: -13.0, exp_count :593935, averged returnover past 100 episodes : -14.72\n",
            "Q loss: 7.606101007695543e-06\n",
            "Episode : 474 , episodic length : 1755, return: -17.0, exp_count :595691, averged returnover past 100 episodes : -14.77\n",
            "Episode : 475 , episodic length : 2061, return: -14.0, exp_count :597753, averged returnover past 100 episodes : -14.76\n",
            "Q loss: 9.088827937375754e-06\n",
            "Episode : 476 , episodic length : 1329, return: -17.0, exp_count :599083, averged returnover past 100 episodes : -14.76\n",
            "Episode : 477 , episodic length : 1524, return: -19.0, exp_count :600608, averged returnover past 100 episodes : -14.77\n",
            "Q loss: 1.4573546650353819e-05\n",
            "Episode : 478 , episodic length : 1902, return: -14.0, exp_count :602511, averged returnover past 100 episodes : -14.78\n",
            "Episode : 479 , episodic length : 1912, return: -18.0, exp_count :604424, averged returnover past 100 episodes : -14.77\n",
            "Q loss: 1.1136668035760522e-05\n",
            "Episode : 480 , episodic length : 1812, return: -13.0, exp_count :606237, averged returnover past 100 episodes : -14.74\n",
            "Episode : 481 , episodic length : 2124, return: -13.0, exp_count :608362, averged returnover past 100 episodes : -14.7\n",
            "Episode : 482 , episodic length : 1490, return: -16.0, exp_count :609853, averged returnover past 100 episodes : -14.7\n",
            "Q loss: 6.58606495562708e-06\n",
            "Episode : 483 , episodic length : 2052, return: -15.0, exp_count :611906, averged returnover past 100 episodes : -14.69\n",
            "Episode : 484 , episodic length : 1774, return: -17.0, exp_count :613681, averged returnover past 100 episodes : -14.69\n",
            "Q loss: 6.9188890847726725e-06\n",
            "Episode : 485 , episodic length : 2014, return: -14.0, exp_count :615696, averged returnover past 100 episodes : -14.68\n",
            "Episode : 486 , episodic length : 2076, return: -13.0, exp_count :617773, averged returnover past 100 episodes : -14.65\n",
            "Q loss: 3.921662482753163e-06\n",
            "Episode : 487 , episodic length : 2302, return: -13.0, exp_count :620076, averged returnover past 100 episodes : -14.62\n",
            "Q loss: 7.5506000030145515e-06\n",
            "Episode : 488 , episodic length : 2429, return: -11.0, exp_count :622506, averged returnover past 100 episodes : -14.61\n",
            "Episode : 489 , episodic length : 1893, return: -16.0, exp_count :624400, averged returnover past 100 episodes : -14.59\n",
            "Q loss: 1.547416832181625e-05\n",
            "Episode : 490 , episodic length : 2235, return: -13.0, exp_count :626636, averged returnover past 100 episodes : -14.54\n",
            "Episode : 491 , episodic length : 1647, return: -18.0, exp_count :628284, averged returnover past 100 episodes : -14.58\n",
            "Q loss: 7.113317224138882e-06\n",
            "Episode : 492 , episodic length : 2128, return: -17.0, exp_count :630413, averged returnover past 100 episodes : -14.6\n",
            "Episode : 493 , episodic length : 2346, return: -15.0, exp_count :632760, averged returnover past 100 episodes : -14.61\n",
            "Q loss: 4.787813395523699e-06\n",
            "Episode : 494 , episodic length : 1727, return: -15.0, exp_count :634488, averged returnover past 100 episodes : -14.59\n",
            "Episode : 495 , episodic length : 2167, return: -15.0, exp_count :636656, averged returnover past 100 episodes : -14.57\n",
            "Q loss: 4.638500286091585e-06\n",
            "Episode : 496 , episodic length : 1785, return: -14.0, exp_count :638442, averged returnover past 100 episodes : -14.53\n",
            "Episode : 497 , episodic length : 1327, return: -19.0, exp_count :639770, averged returnover past 100 episodes : -14.55\n",
            "Episode : 498 , episodic length : 1522, return: -19.0, exp_count :641293, averged returnover past 100 episodes : -14.6\n",
            "Q loss: 4.235108917782782e-06\n",
            "Episode : 499 , episodic length : 1920, return: -15.0, exp_count :643214, averged returnover past 100 episodes : -14.63\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-11. -11. -19. -12. -21. -19. -21. -21. -21. -12. -11. -21. -15. -15.\n",
            " -19.]\n",
            "Episode : 500 , episodic length : 2187, return: -8.0, exp_count :645402, averged returnover past 100 episodes : -14.57\n",
            "Q loss: 4.0784293560136575e-06\n",
            "Episode : 501 , episodic length : 2163, return: -9.0, exp_count :647566, averged returnover past 100 episodes : -14.52\n",
            "Episode : 502 , episodic length : 2261, return: -15.0, exp_count :649828, averged returnover past 100 episodes : -14.5\n",
            "Q loss: 6.083419066271745e-06\n",
            "Episode : 503 , episodic length : 2164, return: -14.0, exp_count :651993, averged returnover past 100 episodes : -14.51\n",
            "Episode : 504 , episodic length : 1532, return: -17.0, exp_count :653526, averged returnover past 100 episodes : -14.53\n",
            "Q loss: 2.945956111943815e-06\n",
            "Episode : 505 , episodic length : 2158, return: -15.0, exp_count :655685, averged returnover past 100 episodes : -14.54\n",
            "Episode : 506 , episodic length : 1998, return: -14.0, exp_count :657684, averged returnover past 100 episodes : -14.53\n",
            "Q loss: 8.567539225623477e-06\n",
            "Episode : 507 , episodic length : 2657, return: -10.0, exp_count :660342, averged returnover past 100 episodes : -14.46\n",
            "Q loss: 6.334706085908692e-06\n",
            "Episode : 508 , episodic length : 2153, return: -15.0, exp_count :662496, averged returnover past 100 episodes : -14.45\n",
            "Episode : 509 , episodic length : 2432, return: -12.0, exp_count :664929, averged returnover past 100 episodes : -14.46\n",
            "Q loss: 4.588622232404305e-06\n",
            "Episode : 510 , episodic length : 2140, return: -13.0, exp_count :667070, averged returnover past 100 episodes : -14.43\n",
            "Episode : 511 , episodic length : 2055, return: -10.0, exp_count :669126, averged returnover past 100 episodes : -14.37\n",
            "Q loss: 9.313835107604973e-06\n",
            "Episode : 512 , episodic length : 1633, return: -14.0, exp_count :670760, averged returnover past 100 episodes : -14.37\n",
            "Episode : 513 , episodic length : 2042, return: -11.0, exp_count :672803, averged returnover past 100 episodes : -14.3\n",
            "Q loss: 6.012379344610963e-06\n",
            "Episode : 514 , episodic length : 1929, return: -16.0, exp_count :674733, averged returnover past 100 episodes : -14.32\n",
            "Episode : 515 , episodic length : 2319, return: -10.0, exp_count :677053, averged returnover past 100 episodes : -14.35\n",
            "Q loss: 3.453633780736709e-06\n",
            "Episode : 516 , episodic length : 1760, return: -13.0, exp_count :678814, averged returnover past 100 episodes : -14.33\n",
            "Episode : 517 , episodic length : 1920, return: -14.0, exp_count :680735, averged returnover past 100 episodes : -14.32\n",
            "Q loss: 3.424949909458519e-06\n",
            "Episode : 518 , episodic length : 2439, return: -12.0, exp_count :683175, averged returnover past 100 episodes : -14.3\n",
            "Episode : 519 , episodic length : 2122, return: -14.0, exp_count :685298, averged returnover past 100 episodes : -14.33\n",
            "Q loss: 7.136837211874081e-06\n",
            "Episode : 520 , episodic length : 1958, return: -15.0, exp_count :687257, averged returnover past 100 episodes : -14.35\n",
            "Episode : 521 , episodic length : 1469, return: -17.0, exp_count :688727, averged returnover past 100 episodes : -14.41\n",
            "Q loss: 4.455779162526596e-06\n",
            "Episode : 522 , episodic length : 1831, return: -18.0, exp_count :690559, averged returnover past 100 episodes : -14.44\n",
            "Episode : 523 , episodic length : 2219, return: -12.0, exp_count :692779, averged returnover past 100 episodes : -14.43\n",
            "Q loss: 4.0085365071718115e-06\n",
            "Episode : 524 , episodic length : 1992, return: -15.0, exp_count :694772, averged returnover past 100 episodes : -14.43\n",
            "Episode : 525 , episodic length : 1966, return: -13.0, exp_count :696739, averged returnover past 100 episodes : -14.4\n",
            "Q loss: 4.024550435133278e-06\n",
            "Episode : 526 , episodic length : 1760, return: -16.0, exp_count :698500, averged returnover past 100 episodes : -14.41\n",
            "Episode : 527 , episodic length : 1680, return: -14.0, exp_count :700181, averged returnover past 100 episodes : -14.42\n",
            "Q loss: 5.631833118968643e-06\n",
            "Episode : 528 , episodic length : 2190, return: -11.0, exp_count :702372, averged returnover past 100 episodes : -14.39\n",
            "Episode : 529 , episodic length : 1432, return: -18.0, exp_count :703805, averged returnover past 100 episodes : -14.38\n",
            "Episode : 530 , episodic length : 1728, return: -14.0, exp_count :705534, averged returnover past 100 episodes : -14.37\n",
            "Q loss: 6.2474696278513875e-06\n",
            "Episode : 531 , episodic length : 2187, return: -10.0, exp_count :707722, averged returnover past 100 episodes : -14.29\n",
            "Episode : 532 , episodic length : 1541, return: -18.0, exp_count :709264, averged returnover past 100 episodes : -14.33\n",
            "Q loss: 6.946921530470718e-06\n",
            "Episode : 533 , episodic length : 1814, return: -14.0, exp_count :711079, averged returnover past 100 episodes : -14.36\n",
            "Episode : 534 , episodic length : 2416, return: -11.0, exp_count :713496, averged returnover past 100 episodes : -14.32\n",
            "Q loss: 4.736220489576226e-06\n",
            "Episode : 535 , episodic length : 1752, return: -16.0, exp_count :715249, averged returnover past 100 episodes : -14.32\n",
            "Episode : 536 , episodic length : 2535, return: -8.0, exp_count :717785, averged returnover past 100 episodes : -14.26\n",
            "Q loss: 3.845609626296209e-06\n",
            "Episode : 537 , episodic length : 2116, return: -13.0, exp_count :719902, averged returnover past 100 episodes : -14.19\n",
            "Episode : 538 , episodic length : 2081, return: -15.0, exp_count :721984, averged returnover past 100 episodes : -14.13\n",
            "Q loss: 5.449321179185063e-06\n",
            "Episode : 539 , episodic length : 2063, return: -13.0, exp_count :724048, averged returnover past 100 episodes : -14.14\n",
            "Episode : 540 , episodic length : 1760, return: -17.0, exp_count :725809, averged returnover past 100 episodes : -14.17\n",
            "Q loss: 4.713237558462424e-06\n",
            "Episode : 541 , episodic length : 2116, return: -9.0, exp_count :727926, averged returnover past 100 episodes : -14.11\n",
            "Episode : 542 , episodic length : 1871, return: -13.0, exp_count :729798, averged returnover past 100 episodes : -14.13\n",
            "Q loss: 7.191239092207979e-06\n",
            "Episode : 543 , episodic length : 2082, return: -10.0, exp_count :731881, averged returnover past 100 episodes : -14.11\n",
            "Episode : 544 , episodic length : 1692, return: -15.0, exp_count :733574, averged returnover past 100 episodes : -14.15\n",
            "Q loss: 2.537638010835508e-06\n",
            "Episode : 545 , episodic length : 1927, return: -16.0, exp_count :735502, averged returnover past 100 episodes : -14.2\n",
            "Episode : 546 , episodic length : 2341, return: -12.0, exp_count :737844, averged returnover past 100 episodes : -14.19\n",
            "Q loss: 1.0135519005416427e-05\n",
            "Episode : 547 , episodic length : 1473, return: -18.0, exp_count :739318, averged returnover past 100 episodes : -14.2\n",
            "Episode : 548 , episodic length : 2116, return: -12.0, exp_count :741435, averged returnover past 100 episodes : -14.16\n",
            "Q loss: 5.590748969552806e-06\n",
            "Episode : 549 , episodic length : 1310, return: -19.0, exp_count :742746, averged returnover past 100 episodes : -14.18\n",
            "Episode : 550 , episodic length : 2384, return: -11.0, exp_count :745131, averged returnover past 100 episodes : -14.13\n",
            "Q loss: 2.7046971808886155e-06\n",
            "Episode : 551 , episodic length : 1961, return: -11.0, exp_count :747093, averged returnover past 100 episodes : -14.12\n",
            "Episode : 552 , episodic length : 2406, return: -10.0, exp_count :749500, averged returnover past 100 episodes : -14.05\n",
            "Q loss: 6.9513203015958425e-06\n",
            "Episode : 553 , episodic length : 1724, return: -15.0, exp_count :751225, averged returnover past 100 episodes : -14.03\n",
            "Episode : 554 , episodic length : 1875, return: -15.0, exp_count :753101, averged returnover past 100 episodes : -14.04\n",
            "Q loss: 4.186795649729902e-06\n",
            "Episode : 555 , episodic length : 2423, return: -9.0, exp_count :755525, averged returnover past 100 episodes : -14.01\n",
            "Episode : 556 , episodic length : 1934, return: -14.0, exp_count :757460, averged returnover past 100 episodes : -14.0\n",
            "Q loss: 6.362377462210134e-06\n",
            "Episode : 557 , episodic length : 2024, return: -14.0, exp_count :759485, averged returnover past 100 episodes : -13.99\n",
            "Episode : 558 , episodic length : 2074, return: -12.0, exp_count :761560, averged returnover past 100 episodes : -13.94\n",
            "Q loss: 5.798993697680999e-06\n",
            "Episode : 559 , episodic length : 1953, return: -13.0, exp_count :763514, averged returnover past 100 episodes : -13.96\n",
            "Q loss: 5.529883310373407e-06\n",
            "Episode : 560 , episodic length : 2682, return: -10.0, exp_count :766197, averged returnover past 100 episodes : -13.89\n",
            "Episode : 561 , episodic length : 1580, return: -17.0, exp_count :767778, averged returnover past 100 episodes : -13.92\n",
            "Episode : 562 , episodic length : 2130, return: -12.0, exp_count :769909, averged returnover past 100 episodes : -13.93\n",
            "Q loss: 3.995614406449022e-06\n",
            "Episode : 563 , episodic length : 2243, return: -12.0, exp_count :772153, averged returnover past 100 episodes : -13.88\n",
            "Q loss: 1.1148757039336488e-05\n",
            "Episode : 564 , episodic length : 2154, return: -13.0, exp_count :774308, averged returnover past 100 episodes : -13.83\n",
            "Episode : 565 , episodic length : 1965, return: -14.0, exp_count :776274, averged returnover past 100 episodes : -13.79\n",
            "Q loss: 7.12504333932884e-06\n",
            "Episode : 566 , episodic length : 1861, return: -16.0, exp_count :778136, averged returnover past 100 episodes : -13.81\n",
            "Episode : 567 , episodic length : 2355, return: -12.0, exp_count :780492, averged returnover past 100 episodes : -13.75\n",
            "Q loss: 2.989293534483295e-06\n",
            "Episode : 568 , episodic length : 2564, return: -10.0, exp_count :783057, averged returnover past 100 episodes : -13.72\n",
            "Episode : 569 , episodic length : 1745, return: -19.0, exp_count :784803, averged returnover past 100 episodes : -13.83\n",
            "Q loss: 6.391554961737711e-06\n",
            "Episode : 570 , episodic length : 1584, return: -17.0, exp_count :786388, averged returnover past 100 episodes : -13.94\n",
            "Episode : 571 , episodic length : 2146, return: -9.0, exp_count :788535, averged returnover past 100 episodes : -13.93\n",
            "Q loss: 5.490975127031561e-06\n",
            "Episode : 572 , episodic length : 1868, return: -15.0, exp_count :790404, averged returnover past 100 episodes : -13.91\n",
            "Episode : 573 , episodic length : 1912, return: -16.0, exp_count :792317, averged returnover past 100 episodes : -13.94\n",
            "Q loss: 1.61292828124715e-05\n",
            "Episode : 574 , episodic length : 1919, return: -14.0, exp_count :794237, averged returnover past 100 episodes : -13.91\n",
            "Episode : 575 , episodic length : 2972, return: -4.0, exp_count :797210, averged returnover past 100 episodes : -13.81\n",
            "Q loss: 3.4544079881015932e-06\n",
            "Episode : 576 , episodic length : 1709, return: -15.0, exp_count :798920, averged returnover past 100 episodes : -13.79\n",
            "Episode : 577 , episodic length : 2246, return: -12.0, exp_count :801167, averged returnover past 100 episodes : -13.72\n",
            "Q loss: 4.723168331111083e-06\n",
            "Episode : 578 , episodic length : 1857, return: -15.0, exp_count :803025, averged returnover past 100 episodes : -13.73\n",
            "Episode : 579 , episodic length : 2197, return: -12.0, exp_count :805223, averged returnover past 100 episodes : -13.67\n",
            "Q loss: 3.3625078685872722e-06\n",
            "Episode : 580 , episodic length : 1758, return: -15.0, exp_count :806982, averged returnover past 100 episodes : -13.69\n",
            "Episode : 581 , episodic length : 2247, return: -10.0, exp_count :809230, averged returnover past 100 episodes : -13.66\n",
            "Q loss: 1.0442816346767358e-05\n",
            "Episode : 582 , episodic length : 2470, return: -8.0, exp_count :811701, averged returnover past 100 episodes : -13.58\n",
            "Episode : 583 , episodic length : 2216, return: -12.0, exp_count :813918, averged returnover past 100 episodes : -13.55\n",
            "Q loss: 3.6250039556762204e-06\n",
            "Episode : 584 , episodic length : 2209, return: -11.0, exp_count :816128, averged returnover past 100 episodes : -13.49\n",
            "Q loss: 3.318440349175944e-06\n",
            "Episode : 585 , episodic length : 2128, return: -12.0, exp_count :818257, averged returnover past 100 episodes : -13.47\n",
            "Episode : 586 , episodic length : 2617, return: -6.0, exp_count :820875, averged returnover past 100 episodes : -13.4\n",
            "Q loss: 4.825938503927318e-06\n",
            "Episode : 587 , episodic length : 2293, return: -14.0, exp_count :823169, averged returnover past 100 episodes : -13.41\n",
            "Episode : 588 , episodic length : 2067, return: -12.0, exp_count :825237, averged returnover past 100 episodes : -13.42\n",
            "Q loss: 4.335709036240587e-06\n",
            "Episode : 589 , episodic length : 2147, return: -10.0, exp_count :827385, averged returnover past 100 episodes : -13.36\n",
            "Episode : 590 , episodic length : 2097, return: -11.0, exp_count :829483, averged returnover past 100 episodes : -13.34\n",
            "Q loss: 5.136864274390973e-06\n",
            "Episode : 591 , episodic length : 1611, return: -14.0, exp_count :831095, averged returnover past 100 episodes : -13.3\n",
            "Episode : 592 , episodic length : 2416, return: -10.0, exp_count :833512, averged returnover past 100 episodes : -13.23\n",
            "Q loss: 5.967551260255277e-06\n",
            "Episode : 593 , episodic length : 2791, return: -11.0, exp_count :836304, averged returnover past 100 episodes : -13.19\n",
            "Q loss: 2.6554885153018404e-06\n",
            "Episode : 594 , episodic length : 2352, return: -13.0, exp_count :838657, averged returnover past 100 episodes : -13.17\n",
            "Q loss: 4.21651202486828e-06\n",
            "Episode : 595 , episodic length : 3377, return: -2.0, exp_count :842035, averged returnover past 100 episodes : -13.04\n",
            "Episode : 596 , episodic length : 2892, return: -4.0, exp_count :844928, averged returnover past 100 episodes : -12.94\n",
            "Q loss: 2.93353923552786e-06\n",
            "Episode : 597 , episodic length : 2047, return: -13.0, exp_count :846976, averged returnover past 100 episodes : -12.88\n",
            "Episode : 598 , episodic length : 2319, return: -9.0, exp_count :849296, averged returnover past 100 episodes : -12.78\n",
            "Q loss: 4.249335233907914e-06\n",
            "Episode : 599 , episodic length : 2117, return: -15.0, exp_count :851414, averged returnover past 100 episodes : -12.78\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-11. -17.  -7. -17. -11.  -7. -11. -11. -17. -17. -11. -11. -11.  -7.\n",
            " -17.]\n",
            "Episode : 600 , episodic length : 1847, return: -14.0, exp_count :853262, averged returnover past 100 episodes : -12.84\n",
            "Q loss: 3.913668024324579e-06\n",
            "Episode : 601 , episodic length : 2625, return: -9.0, exp_count :855888, averged returnover past 100 episodes : -12.84\n",
            "Q loss: 6.019444754201686e-06\n",
            "Episode : 602 , episodic length : 2453, return: -12.0, exp_count :858342, averged returnover past 100 episodes : -12.81\n",
            "Episode : 603 , episodic length : 2047, return: -13.0, exp_count :860390, averged returnover past 100 episodes : -12.8\n",
            "Q loss: 2.0678717191913165e-06\n",
            "Episode : 604 , episodic length : 3221, return: -2.0, exp_count :863612, averged returnover past 100 episodes : -12.65\n",
            "Episode : 605 , episodic length : 1813, return: -14.0, exp_count :865426, averged returnover past 100 episodes : -12.64\n",
            "Q loss: 7.96185304352548e-06\n",
            "Episode : 606 , episodic length : 2176, return: -10.0, exp_count :867603, averged returnover past 100 episodes : -12.6\n",
            "Episode : 607 , episodic length : 1752, return: -17.0, exp_count :869356, averged returnover past 100 episodes : -12.67\n",
            "Q loss: 1.362404873361811e-05\n",
            "Episode : 608 , episodic length : 2456, return: -11.0, exp_count :871813, averged returnover past 100 episodes : -12.63\n",
            "Q loss: 1.2999010323255789e-05\n",
            "Episode : 609 , episodic length : 2919, return: -4.0, exp_count :874733, averged returnover past 100 episodes : -12.55\n",
            "Episode : 610 , episodic length : 2106, return: -12.0, exp_count :876840, averged returnover past 100 episodes : -12.54\n",
            "Q loss: 7.120173904695548e-06\n",
            "Episode : 611 , episodic length : 2207, return: -11.0, exp_count :879048, averged returnover past 100 episodes : -12.55\n",
            "Episode : 612 , episodic length : 2478, return: -8.0, exp_count :881527, averged returnover past 100 episodes : -12.49\n",
            "Q loss: 1.0882693459279835e-05\n",
            "Episode : 613 , episodic length : 1894, return: -16.0, exp_count :883422, averged returnover past 100 episodes : -12.54\n",
            "Q loss: 2.6413781597511843e-06\n",
            "Episode : 614 , episodic length : 2677, return: -10.0, exp_count :886100, averged returnover past 100 episodes : -12.48\n",
            "Episode : 615 , episodic length : 2562, return: -10.0, exp_count :888663, averged returnover past 100 episodes : -12.48\n",
            "Q loss: 7.087135600158945e-06\n",
            "Episode : 616 , episodic length : 2139, return: -15.0, exp_count :890803, averged returnover past 100 episodes : -12.5\n",
            "Episode : 617 , episodic length : 1845, return: -17.0, exp_count :892649, averged returnover past 100 episodes : -12.53\n",
            "Q loss: 4.240321231918642e-06\n",
            "Episode : 618 , episodic length : 2087, return: -15.0, exp_count :894737, averged returnover past 100 episodes : -12.56\n",
            "Episode : 619 , episodic length : 3026, return: -8.0, exp_count :897764, averged returnover past 100 episodes : -12.5\n",
            "Q loss: 5.815448275825474e-06\n",
            "Episode : 620 , episodic length : 2600, return: -9.0, exp_count :900365, averged returnover past 100 episodes : -12.44\n",
            "Q loss: 7.1015679168340284e-06\n",
            "Episode : 621 , episodic length : 2635, return: -10.0, exp_count :903001, averged returnover past 100 episodes : -12.37\n",
            "Episode : 622 , episodic length : 2270, return: -14.0, exp_count :905272, averged returnover past 100 episodes : -12.33\n",
            "Q loss: 6.1526434365077876e-06\n",
            "Episode : 623 , episodic length : 2318, return: -13.0, exp_count :907591, averged returnover past 100 episodes : -12.34\n",
            "Q loss: 3.2011100756790256e-06\n",
            "Episode : 624 , episodic length : 2515, return: -10.0, exp_count :910107, averged returnover past 100 episodes : -12.29\n",
            "Episode : 625 , episodic length : 1997, return: -17.0, exp_count :912105, averged returnover past 100 episodes : -12.33\n",
            "Q loss: 3.5983243833470624e-06\n",
            "Episode : 626 , episodic length : 1992, return: -9.0, exp_count :914098, averged returnover past 100 episodes : -12.26\n",
            "Episode : 627 , episodic length : 2008, return: -10.0, exp_count :916107, averged returnover past 100 episodes : -12.22\n",
            "Q loss: 2.432558630971471e-06\n",
            "Episode : 628 , episodic length : 2435, return: -7.0, exp_count :918543, averged returnover past 100 episodes : -12.18\n",
            "Episode : 629 , episodic length : 1346, return: -18.0, exp_count :919890, averged returnover past 100 episodes : -12.18\n",
            "Episode : 630 , episodic length : 1266, return: -17.0, exp_count :921157, averged returnover past 100 episodes : -12.21\n",
            "Q loss: 5.926557605562266e-06\n",
            "Episode : 631 , episodic length : 1996, return: -15.0, exp_count :923154, averged returnover past 100 episodes : -12.26\n",
            "Episode : 632 , episodic length : 2342, return: -12.0, exp_count :925497, averged returnover past 100 episodes : -12.2\n",
            "Q loss: 2.5619567622925388e-06\n",
            "Episode : 633 , episodic length : 2006, return: -15.0, exp_count :927504, averged returnover past 100 episodes : -12.21\n",
            "Episode : 634 , episodic length : 1679, return: -17.0, exp_count :929184, averged returnover past 100 episodes : -12.27\n",
            "Q loss: 3.70165980712045e-06\n",
            "Episode : 635 , episodic length : 2141, return: -13.0, exp_count :931326, averged returnover past 100 episodes : -12.24\n",
            "Episode : 636 , episodic length : 1891, return: -14.0, exp_count :933218, averged returnover past 100 episodes : -12.3\n",
            "Q loss: 8.987000910565257e-06\n",
            "Episode : 637 , episodic length : 2023, return: -10.0, exp_count :935242, averged returnover past 100 episodes : -12.27\n",
            "Episode : 638 , episodic length : 1739, return: -16.0, exp_count :936982, averged returnover past 100 episodes : -12.28\n",
            "Q loss: 2.7084461180493236e-06\n",
            "Episode : 639 , episodic length : 2041, return: -13.0, exp_count :939024, averged returnover past 100 episodes : -12.28\n",
            "Episode : 640 , episodic length : 2293, return: -9.0, exp_count :941318, averged returnover past 100 episodes : -12.2\n",
            "Q loss: 3.3322780836897437e-06\n",
            "Episode : 641 , episodic length : 2271, return: -11.0, exp_count :943590, averged returnover past 100 episodes : -12.22\n",
            "Episode : 642 , episodic length : 1996, return: -14.0, exp_count :945587, averged returnover past 100 episodes : -12.23\n",
            "Q loss: 9.332623449154198e-06\n",
            "Episode : 643 , episodic length : 2704, return: -6.0, exp_count :948292, averged returnover past 100 episodes : -12.19\n",
            "Q loss: 6.202082659001462e-06\n",
            "Episode : 644 , episodic length : 2138, return: -15.0, exp_count :950431, averged returnover past 100 episodes : -12.19\n",
            "Episode : 645 , episodic length : 2261, return: -14.0, exp_count :952693, averged returnover past 100 episodes : -12.17\n",
            "Q loss: 2.7176436105946777e-06\n",
            "Episode : 646 , episodic length : 2404, return: -10.0, exp_count :955098, averged returnover past 100 episodes : -12.15\n",
            "Episode : 647 , episodic length : 2118, return: -11.0, exp_count :957217, averged returnover past 100 episodes : -12.08\n",
            "Q loss: 3.903570359398145e-06\n",
            "Episode : 648 , episodic length : 2133, return: -13.0, exp_count :959351, averged returnover past 100 episodes : -12.09\n",
            "Episode : 649 , episodic length : 2602, return: -8.0, exp_count :961954, averged returnover past 100 episodes : -11.98\n",
            "Q loss: 7.005777661106549e-06\n",
            "Episode : 650 , episodic length : 2850, return: -9.0, exp_count :964805, averged returnover past 100 episodes : -11.96\n",
            "Q loss: 2.594798615973559e-06\n",
            "Episode : 651 , episodic length : 2065, return: -13.0, exp_count :966871, averged returnover past 100 episodes : -11.98\n",
            "Episode : 652 , episodic length : 2522, return: -10.0, exp_count :969394, averged returnover past 100 episodes : -11.98\n",
            "Q loss: 4.782602900377242e-06\n",
            "Episode : 653 , episodic length : 2942, return: -5.0, exp_count :972337, averged returnover past 100 episodes : -11.88\n",
            "Q loss: 3.175416622980265e-06\n",
            "Episode : 654 , episodic length : 2718, return: -10.0, exp_count :975056, averged returnover past 100 episodes : -11.83\n",
            "Q loss: 3.782919520745054e-06\n",
            "Episode : 655 , episodic length : 3151, return: -1.0, exp_count :978208, averged returnover past 100 episodes : -11.75\n",
            "Episode : 656 , episodic length : 2353, return: -8.0, exp_count :980562, averged returnover past 100 episodes : -11.69\n",
            "Q loss: 3.385146101209102e-06\n",
            "Episode : 657 , episodic length : 2048, return: -13.0, exp_count :982611, averged returnover past 100 episodes : -11.68\n",
            "Episode : 658 , episodic length : 2000, return: -12.0, exp_count :984612, averged returnover past 100 episodes : -11.68\n",
            "Q loss: 3.925435521523468e-06\n",
            "Episode : 659 , episodic length : 2489, return: -8.0, exp_count :987102, averged returnover past 100 episodes : -11.63\n",
            "Episode : 660 , episodic length : 1870, return: -13.0, exp_count :988973, averged returnover past 100 episodes : -11.66\n",
            "Q loss: 3.2454313441121485e-06\n",
            "Episode : 661 , episodic length : 2469, return: -11.0, exp_count :991443, averged returnover past 100 episodes : -11.6\n",
            "Episode : 662 , episodic length : 2093, return: -13.0, exp_count :993537, averged returnover past 100 episodes : -11.61\n",
            "Q loss: 6.661264706053771e-06\n",
            "Episode : 663 , episodic length : 2753, return: -6.0, exp_count :996291, averged returnover past 100 episodes : -11.55\n",
            "Q loss: 2.0350762497400865e-06\n",
            "Episode : 664 , episodic length : 1912, return: -14.0, exp_count :998204, averged returnover past 100 episodes : -11.56\n",
            "Episode : 665 , episodic length : 2912, return: -7.0, exp_count :1001117, averged returnover past 100 episodes : -11.49\n",
            "Q loss: 4.756768248626031e-06\n",
            "Episode : 666 , episodic length : 1631, return: -18.0, exp_count :1002749, averged returnover past 100 episodes : -11.51\n",
            "Episode : 667 , episodic length : 2052, return: -11.0, exp_count :1004802, averged returnover past 100 episodes : -11.5\n",
            "Q loss: 4.338043709140038e-06\n",
            "Episode : 668 , episodic length : 2769, return: -3.0, exp_count :1007572, averged returnover past 100 episodes : -11.43\n",
            "Q loss: 3.1006593417259865e-06\n",
            "Episode : 669 , episodic length : 2468, return: -10.0, exp_count :1010041, averged returnover past 100 episodes : -11.34\n",
            "Episode : 670 , episodic length : 2501, return: -7.0, exp_count :1012543, averged returnover past 100 episodes : -11.24\n",
            "Q loss: 3.7007741866545985e-06\n",
            "Episode : 671 , episodic length : 1877, return: -14.0, exp_count :1014421, averged returnover past 100 episodes : -11.29\n",
            "Episode : 672 , episodic length : 2260, return: -14.0, exp_count :1016682, averged returnover past 100 episodes : -11.28\n",
            "Q loss: 7.460645520040998e-06\n",
            "Episode : 673 , episodic length : 2326, return: -9.0, exp_count :1019009, averged returnover past 100 episodes : -11.21\n",
            "Episode : 674 , episodic length : 2710, return: 5.0, exp_count :1021720, averged returnover past 100 episodes : -11.02\n",
            "Q loss: 5.874543148820521e-06\n",
            "Episode : 675 , episodic length : 1657, return: -15.0, exp_count :1023378, averged returnover past 100 episodes : -11.13\n",
            "Episode : 676 , episodic length : 2441, return: -8.0, exp_count :1025820, averged returnover past 100 episodes : -11.06\n",
            "Q loss: 6.014613518345868e-06\n",
            "Episode : 677 , episodic length : 2492, return: -8.0, exp_count :1028313, averged returnover past 100 episodes : -11.02\n",
            "Q loss: 2.1020982785557862e-06\n",
            "Episode : 678 , episodic length : 2198, return: -13.0, exp_count :1030512, averged returnover past 100 episodes : -11.0\n",
            "Episode : 679 , episodic length : 2612, return: -10.0, exp_count :1033125, averged returnover past 100 episodes : -10.98\n",
            "Q loss: 3.6717019611387514e-06\n",
            "Episode : 680 , episodic length : 2448, return: -10.0, exp_count :1035574, averged returnover past 100 episodes : -10.93\n",
            "Episode : 681 , episodic length : 1995, return: -12.0, exp_count :1037570, averged returnover past 100 episodes : -10.95\n",
            "Q loss: 2.994667738676071e-06\n",
            "Episode : 682 , episodic length : 1854, return: -16.0, exp_count :1039425, averged returnover past 100 episodes : -11.03\n",
            "Episode : 683 , episodic length : 1893, return: -14.0, exp_count :1041319, averged returnover past 100 episodes : -11.05\n",
            "Q loss: 3.7997710933268536e-06\n",
            "Episode : 684 , episodic length : 2720, return: -6.0, exp_count :1044040, averged returnover past 100 episodes : -11.0\n",
            "Q loss: 2.372595190536231e-06\n",
            "Episode : 685 , episodic length : 2715, return: -8.0, exp_count :1046756, averged returnover past 100 episodes : -10.96\n",
            "Episode : 686 , episodic length : 2225, return: -10.0, exp_count :1048982, averged returnover past 100 episodes : -11.0\n",
            "Q loss: 4.957366400049068e-06\n",
            "Episode : 687 , episodic length : 2351, return: -13.0, exp_count :1051334, averged returnover past 100 episodes : -10.99\n",
            "Q loss: 3.2291850402543787e-06\n",
            "Episode : 688 , episodic length : 2853, return: -6.0, exp_count :1054188, averged returnover past 100 episodes : -10.93\n",
            "Episode : 689 , episodic length : 2254, return: -9.0, exp_count :1056443, averged returnover past 100 episodes : -10.92\n",
            "Q loss: 2.979108558065491e-06\n",
            "Episode : 690 , episodic length : 2527, return: -10.0, exp_count :1058971, averged returnover past 100 episodes : -10.91\n",
            "Episode : 691 , episodic length : 2647, return: -12.0, exp_count :1061619, averged returnover past 100 episodes : -10.89\n",
            "Q loss: 2.8848317015217617e-06\n",
            "Episode : 692 , episodic length : 2639, return: -8.0, exp_count :1064259, averged returnover past 100 episodes : -10.87\n",
            "Q loss: 6.138820481282892e-06\n",
            "Episode : 693 , episodic length : 2841, return: -7.0, exp_count :1067101, averged returnover past 100 episodes : -10.83\n",
            "Episode : 694 , episodic length : 2608, return: -9.0, exp_count :1069710, averged returnover past 100 episodes : -10.79\n",
            "Q loss: 3.5413472687650938e-06\n",
            "Episode : 695 , episodic length : 2298, return: -14.0, exp_count :1072009, averged returnover past 100 episodes : -10.91\n",
            "Episode : 696 , episodic length : 806, return: -21.0, exp_count :1072816, averged returnover past 100 episodes : -11.08\n",
            "Q loss: 3.5135424241161672e-06\n",
            "Episode : 697 , episodic length : 2229, return: -10.0, exp_count :1075046, averged returnover past 100 episodes : -11.05\n",
            "Episode : 698 , episodic length : 2652, return: -9.0, exp_count :1077699, averged returnover past 100 episodes : -11.05\n",
            "Q loss: 2.205360488005681e-06\n",
            "Episode : 699 , episodic length : 2345, return: -12.0, exp_count :1080045, averged returnover past 100 episodes : -11.02\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [ 4. -5. -5. -6.  4.  4. -5. -6. -5. -6. -4. -5.  4. -5. -5.]\n",
            "Q loss: 4.16722923546331e-06\n",
            "Episode : 700 , episodic length : 2196, return: -10.0, exp_count :1082242, averged returnover past 100 episodes : -10.98\n",
            "Episode : 701 , episodic length : 2004, return: -13.0, exp_count :1084247, averged returnover past 100 episodes : -11.02\n",
            "Q loss: 2.86507815872028e-06\n",
            "Episode : 702 , episodic length : 2348, return: -10.0, exp_count :1086596, averged returnover past 100 episodes : -11.0\n",
            "Episode : 703 , episodic length : 3304, return: -4.0, exp_count :1089901, averged returnover past 100 episodes : -10.91\n",
            "Q loss: 6.847719305369537e-06\n",
            "Episode : 704 , episodic length : 2358, return: -14.0, exp_count :1092260, averged returnover past 100 episodes : -11.03\n",
            "Q loss: 3.056326931982767e-06\n",
            "Episode : 705 , episodic length : 2128, return: -13.0, exp_count :1094389, averged returnover past 100 episodes : -11.02\n",
            "Episode : 706 , episodic length : 3202, return: -4.0, exp_count :1097592, averged returnover past 100 episodes : -10.96\n",
            "Q loss: 3.1382041925098747e-06\n",
            "Episode : 707 , episodic length : 3077, return: -7.0, exp_count :1100670, averged returnover past 100 episodes : -10.86\n",
            "Q loss: 3.2287991871271515e-06\n",
            "Episode : 708 , episodic length : 3551, return: -5.0, exp_count :1104222, averged returnover past 100 episodes : -10.8\n",
            "Q loss: 2.612642447274993e-06\n",
            "Episode : 709 , episodic length : 3505, return: -6.0, exp_count :1107728, averged returnover past 100 episodes : -10.82\n",
            "Q loss: 4.1277266973338556e-06\n",
            "Episode : 710 , episodic length : 3072, return: -9.0, exp_count :1110801, averged returnover past 100 episodes : -10.79\n",
            "Episode : 711 , episodic length : 2778, return: -11.0, exp_count :1113580, averged returnover past 100 episodes : -10.79\n",
            "Q loss: 2.7390151444706134e-06\n",
            "Episode : 712 , episodic length : 3156, return: -8.0, exp_count :1116737, averged returnover past 100 episodes : -10.79\n",
            "Q loss: 3.554484010237502e-06\n",
            "Episode : 713 , episodic length : 2377, return: -11.0, exp_count :1119115, averged returnover past 100 episodes : -10.74\n",
            "Q loss: 2.864992438844638e-06\n",
            "Episode : 714 , episodic length : 3250, return: -4.0, exp_count :1122366, averged returnover past 100 episodes : -10.68\n",
            "Episode : 715 , episodic length : 2238, return: -12.0, exp_count :1124605, averged returnover past 100 episodes : -10.7\n",
            "Q loss: 2.3996594791242387e-06\n",
            "Episode : 716 , episodic length : 1990, return: -13.0, exp_count :1126596, averged returnover past 100 episodes : -10.68\n",
            "Episode : 717 , episodic length : 2909, return: -8.0, exp_count :1129506, averged returnover past 100 episodes : -10.59\n",
            "Q loss: 1.927599441842176e-06\n",
            "Episode : 718 , episodic length : 2207, return: -15.0, exp_count :1131714, averged returnover past 100 episodes : -10.59\n",
            "Q loss: 3.700838533404749e-06\n",
            "Episode : 719 , episodic length : 2741, return: -11.0, exp_count :1134456, averged returnover past 100 episodes : -10.62\n",
            "Episode : 720 , episodic length : 3127, return: -7.0, exp_count :1137584, averged returnover past 100 episodes : -10.6\n",
            "Q loss: 2.137411456715199e-06\n",
            "Episode : 721 , episodic length : 1632, return: -15.0, exp_count :1139217, averged returnover past 100 episodes : -10.65\n",
            "Q loss: 2.607858732517343e-06\n",
            "Episode : 722 , episodic length : 3417, return: -5.0, exp_count :1142635, averged returnover past 100 episodes : -10.56\n",
            "Episode : 723 , episodic length : 3214, return: -6.0, exp_count :1145850, averged returnover past 100 episodes : -10.49\n",
            "Q loss: 3.3259220799664035e-06\n",
            "Episode : 724 , episodic length : 2543, return: -10.0, exp_count :1148394, averged returnover past 100 episodes : -10.49\n",
            "Q loss: 2.988512278534472e-06\n",
            "Episode : 725 , episodic length : 2708, return: -9.0, exp_count :1151103, averged returnover past 100 episodes : -10.41\n",
            "Episode : 726 , episodic length : 2219, return: -12.0, exp_count :1153323, averged returnover past 100 episodes : -10.44\n",
            "Q loss: 3.148857103951741e-06\n",
            "Episode : 727 , episodic length : 2332, return: -9.0, exp_count :1155656, averged returnover past 100 episodes : -10.43\n",
            "Q loss: 1.0448672583152074e-05\n",
            "Episode : 728 , episodic length : 2673, return: -6.0, exp_count :1158330, averged returnover past 100 episodes : -10.42\n",
            "Episode : 729 , episodic length : 2762, return: -4.0, exp_count :1161093, averged returnover past 100 episodes : -10.28\n",
            "Q loss: 5.228471309237648e-06\n",
            "Episode : 730 , episodic length : 3000, return: -7.0, exp_count :1164094, averged returnover past 100 episodes : -10.18\n",
            "Q loss: 6.739273885614239e-06\n",
            "Episode : 731 , episodic length : 2544, return: -10.0, exp_count :1166639, averged returnover past 100 episodes : -10.13\n",
            "Episode : 732 , episodic length : 1714, return: -15.0, exp_count :1168354, averged returnover past 100 episodes : -10.16\n",
            "Q loss: 4.815135071112309e-06\n",
            "Episode : 733 , episodic length : 2411, return: -11.0, exp_count :1170766, averged returnover past 100 episodes : -10.12\n",
            "Episode : 734 , episodic length : 2384, return: -13.0, exp_count :1173151, averged returnover past 100 episodes : -10.08\n",
            "Q loss: 2.160466237910441e-06\n",
            "Episode : 735 , episodic length : 2565, return: -11.0, exp_count :1175717, averged returnover past 100 episodes : -10.06\n",
            "Q loss: 3.6690657907456625e-06\n",
            "Episode : 736 , episodic length : 2289, return: -10.0, exp_count :1178007, averged returnover past 100 episodes : -10.02\n",
            "Episode : 737 , episodic length : 1611, return: -17.0, exp_count :1179619, averged returnover past 100 episodes : -10.09\n",
            "Q loss: 3.7044419514131732e-06\n",
            "Episode : 738 , episodic length : 2487, return: -10.0, exp_count :1182107, averged returnover past 100 episodes : -10.03\n",
            "Episode : 739 , episodic length : 2280, return: -14.0, exp_count :1184388, averged returnover past 100 episodes : -10.04\n",
            "Q loss: 2.1782473140774528e-06\n",
            "Episode : 740 , episodic length : 2755, return: -7.0, exp_count :1187144, averged returnover past 100 episodes : -10.02\n",
            "Q loss: 2.9183354399719974e-06\n",
            "Episode : 741 , episodic length : 3163, return: -9.0, exp_count :1190308, averged returnover past 100 episodes : -10.0\n",
            "Episode : 742 , episodic length : 2963, return: -5.0, exp_count :1193272, averged returnover past 100 episodes : -9.91\n",
            "Q loss: 4.252982307662023e-06\n",
            "Episode : 743 , episodic length : 2390, return: -13.0, exp_count :1195663, averged returnover past 100 episodes : -9.98\n",
            "Episode : 744 , episodic length : 2062, return: -12.0, exp_count :1197726, averged returnover past 100 episodes : -9.95\n",
            "Q loss: 5.1354403694858775e-05\n",
            "Episode : 745 , episodic length : 2307, return: -10.0, exp_count :1200034, averged returnover past 100 episodes : -9.91\n",
            "Q loss: 4.078051460965071e-06\n",
            "Episode : 746 , episodic length : 2325, return: -11.0, exp_count :1202360, averged returnover past 100 episodes : -9.92\n",
            "Episode : 747 , episodic length : 2809, return: -8.0, exp_count :1205170, averged returnover past 100 episodes : -9.89\n",
            "Q loss: 2.40245071836398e-06\n",
            "Episode : 748 , episodic length : 3195, return: -8.0, exp_count :1208366, averged returnover past 100 episodes : -9.84\n",
            "Q loss: 3.6010192161484156e-06\n",
            "Episode : 749 , episodic length : 2889, return: -5.0, exp_count :1211256, averged returnover past 100 episodes : -9.81\n",
            "Q loss: 3.1722690891911043e-06\n",
            "Episode : 750 , episodic length : 2920, return: -6.0, exp_count :1214177, averged returnover past 100 episodes : -9.78\n",
            "Episode : 751 , episodic length : 3045, return: -10.0, exp_count :1217223, averged returnover past 100 episodes : -9.75\n",
            "Q loss: 4.90448337586713e-06\n",
            "Episode : 752 , episodic length : 3235, return: -5.0, exp_count :1220459, averged returnover past 100 episodes : -9.7\n",
            "Q loss: 2.494921318429988e-06\n",
            "Episode : 753 , episodic length : 2480, return: -14.0, exp_count :1222940, averged returnover past 100 episodes : -9.79\n",
            "Q loss: 4.932172487315256e-06\n",
            "Episode : 754 , episodic length : 3141, return: -6.0, exp_count :1226082, averged returnover past 100 episodes : -9.75\n",
            "Episode : 755 , episodic length : 2849, return: -9.0, exp_count :1228932, averged returnover past 100 episodes : -9.83\n",
            "Q loss: 4.743116733152419e-06\n",
            "Episode : 756 , episodic length : 2721, return: -7.0, exp_count :1231654, averged returnover past 100 episodes : -9.82\n",
            "Q loss: 4.512633040576475e-06\n",
            "Episode : 757 , episodic length : 3402, return: -2.0, exp_count :1235057, averged returnover past 100 episodes : -9.71\n",
            "Q loss: 2.4382079573115334e-06\n",
            "Episode : 758 , episodic length : 3559, return: -3.0, exp_count :1238617, averged returnover past 100 episodes : -9.62\n",
            "Episode : 759 , episodic length : 3051, return: -6.0, exp_count :1241669, averged returnover past 100 episodes : -9.6\n",
            "Q loss: 6.533670784847345e-06\n",
            "Episode : 760 , episodic length : 2554, return: -8.0, exp_count :1244224, averged returnover past 100 episodes : -9.55\n",
            "Q loss: 5.133701961312909e-06\n",
            "Episode : 761 , episodic length : 2827, return: -8.0, exp_count :1247052, averged returnover past 100 episodes : -9.52\n",
            "Q loss: 1.960082954610698e-06\n",
            "Episode : 762 , episodic length : 3260, return: -3.0, exp_count :1250313, averged returnover past 100 episodes : -9.42\n",
            "Episode : 763 , episodic length : 3294, return: -8.0, exp_count :1253608, averged returnover past 100 episodes : -9.44\n",
            "Q loss: 3.813388048001798e-06\n",
            "Episode : 764 , episodic length : 1882, return: -12.0, exp_count :1255491, averged returnover past 100 episodes : -9.42\n",
            "Q loss: 2.1978612494422123e-06\n",
            "Episode : 765 , episodic length : 3036, return: -9.0, exp_count :1258528, averged returnover past 100 episodes : -9.44\n",
            "Episode : 766 , episodic length : 3262, return: -4.0, exp_count :1261791, averged returnover past 100 episodes : -9.3\n",
            "Q loss: 2.8915194434375735e-06\n",
            "Episode : 767 , episodic length : 2133, return: -14.0, exp_count :1263925, averged returnover past 100 episodes : -9.33\n",
            "Episode : 768 , episodic length : 1471, return: -18.0, exp_count :1265397, averged returnover past 100 episodes : -9.48\n",
            "Q loss: 3.137814019282814e-06\n",
            "Episode : 769 , episodic length : 2450, return: -9.0, exp_count :1267848, averged returnover past 100 episodes : -9.47\n",
            "Q loss: 4.575743332679849e-06\n",
            "Episode : 770 , episodic length : 3219, return: -5.0, exp_count :1271068, averged returnover past 100 episodes : -9.45\n",
            "Q loss: 3.631826984928921e-06\n",
            "Episode : 771 , episodic length : 3013, return: -7.0, exp_count :1274082, averged returnover past 100 episodes : -9.38\n",
            "Episode : 772 , episodic length : 3030, return: -7.0, exp_count :1277113, averged returnover past 100 episodes : -9.31\n",
            "Q loss: 4.686161446443293e-06\n",
            "Episode : 773 , episodic length : 3190, return: -2.0, exp_count :1280304, averged returnover past 100 episodes : -9.24\n",
            "Q loss: 3.063692020077724e-06\n",
            "Episode : 774 , episodic length : 3423, return: -7.0, exp_count :1283728, averged returnover past 100 episodes : -9.36\n",
            "Q loss: 2.720639713515993e-06\n",
            "Episode : 775 , episodic length : 3446, return: -4.0, exp_count :1287175, averged returnover past 100 episodes : -9.25\n",
            "Episode : 776 , episodic length : 2179, return: -10.0, exp_count :1289355, averged returnover past 100 episodes : -9.27\n",
            "Q loss: 1.3942716577730607e-05\n",
            "Episode : 777 , episodic length : 2841, return: -6.0, exp_count :1292197, averged returnover past 100 episodes : -9.25\n",
            "Q loss: 2.1498849491763394e-06\n",
            "Episode : 778 , episodic length : 2800, return: -6.0, exp_count :1294998, averged returnover past 100 episodes : -9.18\n",
            "Episode : 779 , episodic length : 2885, return: -3.0, exp_count :1297884, averged returnover past 100 episodes : -9.11\n",
            "Q loss: 2.6241202704113675e-06\n",
            "Episode : 780 , episodic length : 2567, return: 8.0, exp_count :1300452, averged returnover past 100 episodes : -8.93\n",
            "Q loss: 2.9235864076326834e-06\n",
            "Episode : 781 , episodic length : 2936, return: -7.0, exp_count :1303389, averged returnover past 100 episodes : -8.88\n",
            "Episode : 782 , episodic length : 2559, return: -8.0, exp_count :1305949, averged returnover past 100 episodes : -8.8\n",
            "Q loss: 2.506647206246271e-06\n",
            "Episode : 783 , episodic length : 3165, return: -7.0, exp_count :1309115, averged returnover past 100 episodes : -8.73\n",
            "Q loss: 7.300597189896507e-06\n",
            "Episode : 784 , episodic length : 2771, return: -6.0, exp_count :1311887, averged returnover past 100 episodes : -8.73\n",
            "Q loss: 2.088609562633792e-06\n",
            "Episode : 785 , episodic length : 2795, return: -5.0, exp_count :1314683, averged returnover past 100 episodes : -8.7\n",
            "Episode : 786 , episodic length : 2496, return: -10.0, exp_count :1317180, averged returnover past 100 episodes : -8.7\n",
            "Q loss: 4.0959689613373484e-06\n",
            "Episode : 787 , episodic length : 2710, return: -9.0, exp_count :1319891, averged returnover past 100 episodes : -8.66\n",
            "Q loss: 1.2221046745253261e-05\n",
            "Episode : 788 , episodic length : 2701, return: -14.0, exp_count :1322593, averged returnover past 100 episodes : -8.74\n",
            "Episode : 789 , episodic length : 2676, return: -9.0, exp_count :1325270, averged returnover past 100 episodes : -8.74\n",
            "Q loss: 4.112881470064167e-06\n",
            "Episode : 790 , episodic length : 3485, return: -2.0, exp_count :1328756, averged returnover past 100 episodes : -8.66\n",
            "Q loss: 2.0353070340206614e-06\n",
            "Episode : 791 , episodic length : 2577, return: -10.0, exp_count :1331334, averged returnover past 100 episodes : -8.64\n",
            "Q loss: 2.580816499175853e-06\n",
            "Episode : 792 , episodic length : 3879, return: 1.0, exp_count :1335214, averged returnover past 100 episodes : -8.55\n",
            "Q loss: 2.6682537281885743e-06\n",
            "Episode : 793 , episodic length : 3174, return: -1.0, exp_count :1338389, averged returnover past 100 episodes : -8.49\n",
            "Episode : 794 , episodic length : 2810, return: -6.0, exp_count :1341200, averged returnover past 100 episodes : -8.46\n",
            "Q loss: 1.913384494400816e-06\n",
            "Episode : 795 , episodic length : 2911, return: 3.0, exp_count :1344112, averged returnover past 100 episodes : -8.29\n",
            "Q loss: 2.5798647129704477e-06\n",
            "Episode : 796 , episodic length : 3106, return: -4.0, exp_count :1347219, averged returnover past 100 episodes : -8.12\n",
            "Episode : 797 , episodic length : 2440, return: -9.0, exp_count :1349660, averged returnover past 100 episodes : -8.11\n",
            "Q loss: 3.985584044130519e-06\n",
            "Episode : 798 , episodic length : 2953, return: -7.0, exp_count :1352614, averged returnover past 100 episodes : -8.09\n",
            "Q loss: 2.8959466362721287e-06\n",
            "Episode : 799 , episodic length : 3217, return: -4.0, exp_count :1355832, averged returnover past 100 episodes : -8.01\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [-4. -6. -4.  6. -1. -4. -1. -4. -6. -4.  6. -4. -4. -1. -6.]\n",
            "Q loss: 2.6141101443499792e-06\n",
            "Episode : 800 , episodic length : 2850, return: -5.0, exp_count :1358683, averged returnover past 100 episodes : -7.96\n",
            "Episode : 801 , episodic length : 2799, return: -6.0, exp_count :1361483, averged returnover past 100 episodes : -7.89\n",
            "Q loss: 1.9147580587741686e-06\n",
            "Episode : 802 , episodic length : 3159, return: 5.0, exp_count :1364643, averged returnover past 100 episodes : -7.74\n",
            "Q loss: 1.7198253772221506e-06\n",
            "Episode : 803 , episodic length : 3381, return: 2.0, exp_count :1368025, averged returnover past 100 episodes : -7.68\n",
            "Q loss: 3.525798092596233e-06\n",
            "Episode : 804 , episodic length : 3013, return: -2.0, exp_count :1371039, averged returnover past 100 episodes : -7.56\n",
            "Episode : 805 , episodic length : 2720, return: -5.0, exp_count :1373760, averged returnover past 100 episodes : -7.48\n",
            "Q loss: 2.4992646103783045e-06\n",
            "Episode : 806 , episodic length : 2510, return: -9.0, exp_count :1376271, averged returnover past 100 episodes : -7.53\n",
            "Q loss: 1.895205400614941e-06\n",
            "Episode : 807 , episodic length : 3270, return: -3.0, exp_count :1379542, averged returnover past 100 episodes : -7.49\n",
            "Q loss: 2.4115606720442884e-06\n",
            "Episode : 808 , episodic length : 2690, return: 7.0, exp_count :1382233, averged returnover past 100 episodes : -7.37\n",
            "Episode : 809 , episodic length : 2717, return: -7.0, exp_count :1384951, averged returnover past 100 episodes : -7.38\n",
            "Q loss: 2.3137217795010656e-06\n",
            "Episode : 810 , episodic length : 2392, return: -6.0, exp_count :1387344, averged returnover past 100 episodes : -7.35\n",
            "Q loss: 2.513403842385742e-06\n",
            "Episode : 811 , episodic length : 2773, return: -7.0, exp_count :1390118, averged returnover past 100 episodes : -7.31\n",
            "Episode : 812 , episodic length : 3017, return: 6.0, exp_count :1393136, averged returnover past 100 episodes : -7.17\n",
            "Q loss: 2.606685484352056e-06\n",
            "Episode : 813 , episodic length : 2835, return: -3.0, exp_count :1395972, averged returnover past 100 episodes : -7.09\n",
            "Q loss: 2.1723359168390743e-05\n",
            "Episode : 814 , episodic length : 3643, return: 4.0, exp_count :1399616, averged returnover past 100 episodes : -7.01\n",
            "Q loss: 2.348870566493133e-06\n",
            "Episode : 815 , episodic length : 2557, return: 10.0, exp_count :1402174, averged returnover past 100 episodes : -6.79\n",
            "Episode : 816 , episodic length : 3098, return: 2.0, exp_count :1405273, averged returnover past 100 episodes : -6.64\n",
            "Q loss: 3.461135747784283e-06\n",
            "Episode : 817 , episodic length : 2667, return: -5.0, exp_count :1407941, averged returnover past 100 episodes : -6.61\n",
            "Q loss: 1.8917651232186472e-06\n",
            "Episode : 818 , episodic length : 2322, return: 11.0, exp_count :1410264, averged returnover past 100 episodes : -6.35\n",
            "Episode : 819 , episodic length : 2026, return: -13.0, exp_count :1412291, averged returnover past 100 episodes : -6.37\n",
            "Q loss: 2.219987891294295e-06\n",
            "Episode : 820 , episodic length : 3313, return: 1.0, exp_count :1415605, averged returnover past 100 episodes : -6.29\n",
            "Q loss: 1.6443243566754973e-06\n",
            "Episode : 821 , episodic length : 2855, return: 1.0, exp_count :1418461, averged returnover past 100 episodes : -6.13\n",
            "Episode : 822 , episodic length : 2871, return: -8.0, exp_count :1421333, averged returnover past 100 episodes : -6.16\n",
            "Q loss: 3.2130226372828474e-06\n",
            "Episode : 823 , episodic length : 3339, return: -1.0, exp_count :1424673, averged returnover past 100 episodes : -6.11\n",
            "Q loss: 7.628761068190215e-06\n",
            "Episode : 824 , episodic length : 2595, return: 5.0, exp_count :1427269, averged returnover past 100 episodes : -5.96\n",
            "Q loss: 2.404423867119476e-06\n",
            "Episode : 825 , episodic length : 2969, return: 9.0, exp_count :1430239, averged returnover past 100 episodes : -5.78\n",
            "Episode : 826 , episodic length : 2515, return: -10.0, exp_count :1432755, averged returnover past 100 episodes : -5.76\n",
            "Q loss: 1.4955182905396214e-06\n",
            "Episode : 827 , episodic length : 3227, return: -1.0, exp_count :1435983, averged returnover past 100 episodes : -5.68\n",
            "Q loss: 1.40062979880895e-06\n",
            "Episode : 828 , episodic length : 3337, return: 5.0, exp_count :1439321, averged returnover past 100 episodes : -5.57\n",
            "Episode : 829 , episodic length : 2409, return: 10.0, exp_count :1441731, averged returnover past 100 episodes : -5.43\n",
            "Q loss: 1.8881705727835651e-06\n",
            "Episode : 830 , episodic length : 2667, return: 10.0, exp_count :1444399, averged returnover past 100 episodes : -5.26\n",
            "Q loss: 3.942504008591641e-06\n",
            "Episode : 831 , episodic length : 2054, return: 16.0, exp_count :1446454, averged returnover past 100 episodes : -5.0\n",
            "Episode : 832 , episodic length : 2732, return: 7.0, exp_count :1449187, averged returnover past 100 episodes : -4.78\n",
            "Q loss: 2.8518638828245457e-06\n",
            "Episode : 833 , episodic length : 2606, return: 10.0, exp_count :1451794, averged returnover past 100 episodes : -4.57\n",
            "Q loss: 2.0908239548589336e-06\n",
            "Episode : 834 , episodic length : 2677, return: -5.0, exp_count :1454472, averged returnover past 100 episodes : -4.49\n",
            "Episode : 835 , episodic length : 2825, return: 3.0, exp_count :1457298, averged returnover past 100 episodes : -4.35\n",
            "Q loss: 1.964526063602534e-06\n",
            "Episode : 836 , episodic length : 2886, return: 8.0, exp_count :1460185, averged returnover past 100 episodes : -4.17\n",
            "Q loss: 2.7523365133674815e-06\n",
            "Episode : 837 , episodic length : 2805, return: 8.0, exp_count :1462991, averged returnover past 100 episodes : -3.92\n",
            "Episode : 838 , episodic length : 2595, return: -5.0, exp_count :1465587, averged returnover past 100 episodes : -3.87\n",
            "Q loss: 2.3732222871331032e-06\n",
            "Episode : 839 , episodic length : 3390, return: -1.0, exp_count :1468978, averged returnover past 100 episodes : -3.74\n",
            "Q loss: 1.8900918803410605e-06\n",
            "Episode : 840 , episodic length : 2987, return: -1.0, exp_count :1471966, averged returnover past 100 episodes : -3.68\n",
            "Q loss: 2.5072622520383447e-06\n",
            "Episode : 841 , episodic length : 2830, return: 7.0, exp_count :1474797, averged returnover past 100 episodes : -3.52\n",
            "Episode : 842 , episodic length : 2854, return: 6.0, exp_count :1477652, averged returnover past 100 episodes : -3.41\n",
            "Q loss: 3.5028663205594057e-06\n",
            "Episode : 843 , episodic length : 2738, return: 8.0, exp_count :1480391, averged returnover past 100 episodes : -3.2\n",
            "Q loss: 3.6584447116183583e-06\n",
            "Episode : 844 , episodic length : 2755, return: -7.0, exp_count :1483147, averged returnover past 100 episodes : -3.15\n",
            "Q loss: 3.9385358832078055e-06\n",
            "Episode : 845 , episodic length : 2925, return: 8.0, exp_count :1486073, averged returnover past 100 episodes : -2.97\n",
            "Episode : 846 , episodic length : 2624, return: 11.0, exp_count :1488698, averged returnover past 100 episodes : -2.75\n",
            "Q loss: 2.090407178911846e-06\n",
            "Episode : 847 , episodic length : 2798, return: 9.0, exp_count :1491497, averged returnover past 100 episodes : -2.58\n",
            "Q loss: 6.71120869810693e-06\n",
            "Episode : 848 , episodic length : 2980, return: 4.0, exp_count :1494478, averged returnover past 100 episodes : -2.46\n",
            "Episode : 849 , episodic length : 2739, return: 4.0, exp_count :1497218, averged returnover past 100 episodes : -2.37\n",
            "Q loss: 2.4195371679525124e-06\n",
            "Episode : 850 , episodic length : 3051, return: -4.0, exp_count :1500270, averged returnover past 100 episodes : -2.35\n",
            "Q loss: 8.726784471946303e-06\n",
            "Episode : 851 , episodic length : 2583, return: 8.0, exp_count :1502854, averged returnover past 100 episodes : -2.17\n",
            "Episode : 852 , episodic length : 3131, return: 1.0, exp_count :1505986, averged returnover past 100 episodes : -2.11\n",
            "Q loss: 1.3599457133750548e-06\n",
            "Episode : 853 , episodic length : 3400, return: 4.0, exp_count :1509387, averged returnover past 100 episodes : -1.93\n",
            "Q loss: 2.25082112592645e-06\n",
            "Episode : 854 , episodic length : 2246, return: 17.0, exp_count :1511634, averged returnover past 100 episodes : -1.7\n",
            "Q loss: 1.6881378996913554e-06\n",
            "Episode : 855 , episodic length : 2993, return: -3.0, exp_count :1514628, averged returnover past 100 episodes : -1.64\n",
            "Episode : 856 , episodic length : 2394, return: -7.0, exp_count :1517023, averged returnover past 100 episodes : -1.64\n",
            "Q loss: 4.551149686449207e-06\n",
            "Episode : 857 , episodic length : 3261, return: 1.0, exp_count :1520285, averged returnover past 100 episodes : -1.61\n",
            "Q loss: 3.0262460768426536e-06\n",
            "Episode : 858 , episodic length : 2514, return: 13.0, exp_count :1522800, averged returnover past 100 episodes : -1.45\n",
            "Episode : 859 , episodic length : 2345, return: 12.0, exp_count :1525146, averged returnover past 100 episodes : -1.27\n",
            "Q loss: 2.584593175924965e-06\n",
            "Episode : 860 , episodic length : 2889, return: 3.0, exp_count :1528036, averged returnover past 100 episodes : -1.16\n",
            "Q loss: 3.1908025448501576e-06\n",
            "Episode : 861 , episodic length : 2465, return: 12.0, exp_count :1530502, averged returnover past 100 episodes : -0.96\n",
            "Episode : 862 , episodic length : 2376, return: 12.0, exp_count :1532879, averged returnover past 100 episodes : -0.81\n",
            "Q loss: 2.3474276531487703e-06\n",
            "Episode : 863 , episodic length : 3127, return: -3.0, exp_count :1536007, averged returnover past 100 episodes : -0.76\n",
            "Q loss: 2.307201839357731e-06\n",
            "Episode : 864 , episodic length : 3151, return: -1.0, exp_count :1539159, averged returnover past 100 episodes : -0.65\n",
            "Episode : 865 , episodic length : 2033, return: 18.0, exp_count :1541193, averged returnover past 100 episodes : -0.38\n",
            "Q loss: 3.19716491503641e-06\n",
            "Episode : 866 , episodic length : 2819, return: -4.0, exp_count :1544013, averged returnover past 100 episodes : -0.38\n",
            "Q loss: 1.6088106349343434e-06\n",
            "Episode : 867 , episodic length : 2942, return: 6.0, exp_count :1546956, averged returnover past 100 episodes : -0.18\n",
            "Episode : 868 , episodic length : 2172, return: 10.0, exp_count :1549129, averged returnover past 100 episodes : 0.1\n",
            "Q loss: 1.6170729395525996e-06\n",
            "Episode : 869 , episodic length : 3196, return: 5.0, exp_count :1552326, averged returnover past 100 episodes : 0.24\n",
            "Q loss: 1.555461494717747e-06\n",
            "Episode : 870 , episodic length : 2836, return: 6.0, exp_count :1555163, averged returnover past 100 episodes : 0.35\n",
            "Episode : 871 , episodic length : 2293, return: 13.0, exp_count :1557457, averged returnover past 100 episodes : 0.55\n",
            "Q loss: 2.299000243510818e-06\n",
            "Episode : 872 , episodic length : 2419, return: 11.0, exp_count :1559877, averged returnover past 100 episodes : 0.73\n",
            "Q loss: 1.489385795139242e-06\n",
            "Episode : 873 , episodic length : 2967, return: 6.0, exp_count :1562845, averged returnover past 100 episodes : 0.81\n",
            "Q loss: 5.6864523685362656e-06\n",
            "Episode : 874 , episodic length : 3265, return: 4.0, exp_count :1566111, averged returnover past 100 episodes : 0.92\n",
            "Episode : 875 , episodic length : 2760, return: 9.0, exp_count :1568872, averged returnover past 100 episodes : 1.05\n",
            "Q loss: 2.112514494001516e-06\n",
            "Episode : 876 , episodic length : 3010, return: 2.0, exp_count :1571883, averged returnover past 100 episodes : 1.17\n",
            "Q loss: 1.795040930119285e-06\n",
            "Episode : 877 , episodic length : 2981, return: -2.0, exp_count :1574865, averged returnover past 100 episodes : 1.21\n",
            "Q loss: 2.1492380710697034e-06\n",
            "Episode : 878 , episodic length : 3332, return: 2.0, exp_count :1578198, averged returnover past 100 episodes : 1.29\n",
            "Episode : 879 , episodic length : 2123, return: 16.0, exp_count :1580322, averged returnover past 100 episodes : 1.48\n",
            "Q loss: 1.6547166978853056e-06\n",
            "Episode : 880 , episodic length : 2550, return: 13.0, exp_count :1582873, averged returnover past 100 episodes : 1.53\n",
            "Episode : 881 , episodic length : 2334, return: 14.0, exp_count :1585208, averged returnover past 100 episodes : 1.74\n",
            "Q loss: 1.7092886537284357e-06\n",
            "Episode : 882 , episodic length : 2744, return: 9.0, exp_count :1587953, averged returnover past 100 episodes : 1.91\n",
            "Q loss: 1.8656246538739651e-06\n",
            "Episode : 883 , episodic length : 2071, return: 14.0, exp_count :1590025, averged returnover past 100 episodes : 2.12\n",
            "Episode : 884 , episodic length : 2870, return: 8.0, exp_count :1592896, averged returnover past 100 episodes : 2.26\n",
            "Q loss: 1.929568270497839e-06\n",
            "Episode : 885 , episodic length : 2770, return: 11.0, exp_count :1595667, averged returnover past 100 episodes : 2.42\n",
            "Q loss: 2.3155675989983138e-06\n",
            "Episode : 886 , episodic length : 2852, return: 8.0, exp_count :1598520, averged returnover past 100 episodes : 2.6\n",
            "Episode : 887 , episodic length : 3216, return: 3.0, exp_count :1601737, averged returnover past 100 episodes : 2.72\n",
            "Q loss: 3.1587853754899697e-06\n",
            "Episode : 888 , episodic length : 3417, return: 3.0, exp_count :1605155, averged returnover past 100 episodes : 2.89\n",
            "Q loss: 2.868652245524572e-06\n",
            "Episode : 889 , episodic length : 3219, return: 1.0, exp_count :1608375, averged returnover past 100 episodes : 2.99\n",
            "Q loss: 1.5336022443079855e-06\n",
            "Episode : 890 , episodic length : 3097, return: 4.0, exp_count :1611473, averged returnover past 100 episodes : 3.05\n",
            "Q loss: 2.1060047856735764e-06\n",
            "Episode : 891 , episodic length : 2671, return: 9.0, exp_count :1614145, averged returnover past 100 episodes : 3.24\n",
            "Episode : 892 , episodic length : 3795, return: 1.0, exp_count :1617941, averged returnover past 100 episodes : 3.24\n",
            "Q loss: 4.468728548090439e-06\n",
            "Episode : 893 , episodic length : 3577, return: -2.0, exp_count :1621519, averged returnover past 100 episodes : 3.23\n",
            "Q loss: 1.8816192550730193e-06\n",
            "Episode : 894 , episodic length : 2839, return: 9.0, exp_count :1624359, averged returnover past 100 episodes : 3.38\n",
            "Q loss: 2.6018838070740458e-06\n",
            "Episode : 895 , episodic length : 2210, return: 16.0, exp_count :1626570, averged returnover past 100 episodes : 3.51\n",
            "Episode : 896 , episodic length : 2807, return: 7.0, exp_count :1629378, averged returnover past 100 episodes : 3.62\n",
            "Q loss: 1.0639509127940983e-05\n",
            "Episode : 897 , episodic length : 3218, return: 7.0, exp_count :1632597, averged returnover past 100 episodes : 3.78\n",
            "Q loss: 1.4479614947049413e-06\n",
            "Episode : 898 , episodic length : 2730, return: 12.0, exp_count :1635328, averged returnover past 100 episodes : 3.97\n",
            "Episode : 899 , episodic length : 2492, return: 9.0, exp_count :1637821, averged returnover past 100 episodes : 4.1\n",
            "-------start Testing--------\n",
            "Testing result, Returns: [20. 19. 20. 20. 19. 20. 21. 21. 19. 19. 19. 19. 20. 19. 20.]\n",
            "Q loss: 1.5512111986026866e-06\n",
            "Episode : 900 , episodic length : 2344, return: 13.0, exp_count :1640166, averged returnover past 100 episodes : 4.28\n",
            "Q loss: 2.0231527741998434e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}